{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLEXible tutorial: Text classification using PyTorch\n",
    "\n",
    "FLEXible is a library to federate models. We offer the tools to load and federate data or to load federated data, and the tools to create a federated environment. The user must define the model and the *communication primitives* to train the model in a federated environment. This primitives can be expressed in the following steps:\n",
    "- initialization: Initialize the model in the server.\n",
    "- deplot model: Deploy the model to the clients.\n",
    "- training: Define the train function.\n",
    "- collect the weights: Collect the weights of the clients params to aggregate them later.\n",
    "- aggregate the weights: Use an aggregation method to aggregte the collected weights.\n",
    "- deploy model: Deploy the model with the updated weights to the clients.\n",
    "- evaluate: Define the evaluate function.\n",
    "\n",
    "In this notebook, we show how to implement this primitives and how to use FLEXible in orther to federate a model using TensorFlow. In this way, we will train a model using multiple clients, but without sharing any data between clients. We will follow this [tutorial](https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html#) from the PyTorch tutorials for text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from datasets.load import load_dataset\n",
    "from torch import nn\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch we need to define the device where we are going to train and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\") # For MAC_OS GPU\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual in every experiment, the first step is to load the dataset we will use. In this case we will use the dataset **AG_News** for a supervised text classification model. In this tutorial we will have to tokenize the data in orther to build the vocab, as we will build a LSTM model from scratch.\n",
    "\n",
    "We will use torchtext to load the tokenizer and to build the vocabulary from the dataset.\n",
    "For downloading the dataset, we will use the [datasets package](https://huggingface.co/datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english') # Get the tokenizer\n",
    "\n",
    "ag_news_dataset = load_dataset('ag_news', split='test') # Get the dataset from huggingface library\n",
    "train_data = ag_news_dataset['train'] # Get the train data\n",
    "test_data = ag_news_dataset['test'] # Get the test data\n",
    "\n",
    "dataset_ag_news = AG_NEWS(split='train')\n",
    "\n",
    "# Function to help us building the vocabulary\n",
    "def yield_tokens_hf(data):\n",
    "    for text in data['text']:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab_hf = build_vocab_from_iterator(yield_tokens_hf(train_data), specials=[\"<unk>\"])\n",
    "vocab_hf.set_default_index(vocab_hf[\"<unk>\"])\n",
    "\n",
    "train_examples, train_labels = np.array(train_data['text']), np.array(train_data['label'])\n",
    "test_examples, test_labels = np.array(test_data['text']), np.array(test_data['label'])\n",
    "\n",
    "print(f\"Training entries: {len(train_examples)}, test entries: {len(test_examples)}\")\n",
    "\n",
    "# Auxiliar functions to help us with the collate function.\n",
    "text_pipeline = lambda x: vocab_hf(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) From centralized data to federated data\n",
    "Firstly and foremost, we need to encapsulare our centralized dataset as numpy arrays in a FlexDataObject, to split it for every federated client.\n",
    "As we are using a centrilized dataset, we have to federate it. To federate the data we need to create a basic data object for FLEXible that is called **FlexDataObject**. To create a  **FlexDataObject** we need to have the data as *numpy.arrays*.\n",
    "\n",
    "The dataset AG_News is available in both TorchText and HuggingFace datasets, so we can use both to load the data as a FlexDataObject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flex.data import FlexDataObject\n",
    "\n",
    "flex_data = FlexDataObject.from_torchtext_dataset(dataset_ag_news)\n",
    "\n",
    "flex_data = FlexDataObject.from_huggingface_dataset(hf_dataset=train_data, X_columns='text', label_column='label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to federate our dataset, we need to specify how we want to split it among clients in a ``FlexDatasetConfig`` object. For this case we want to split it evenly between 2 clients, that is, an iid distribution. To apply our config to our centralized dataset, we use ``FlexDataDistribution.from_config``. A more complete description of the configuration options of ``FlexDatasetConfig`` to federate a dataset can be found in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flex.data import FlexDatasetConfig, FlexDataDistribution\n",
    "\n",
    "config = FlexDatasetConfig(seed=0)\n",
    "config.n_clients = 2\n",
    "config.replacement = False # ensure that clients do not share any data\n",
    "config.client_names = ['client1', 'client2'] # Optional\n",
    "flex_dataset = FlexDataDistribution.from_config(cdata=flex_data, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there is a shortcut, if we want to split the dataset iid between the clients we can directly use ``FlexDataDistribution.iid_distribution`` with the number of clients and our centralized data stored in a ``FlexDataObject``. Note that in this case the name of the clients are generated automatically: client number ``i`` gets id: ``f\"client_{i}\"``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flex.data import FlexDataDistribution\n",
    "\n",
    "flex_dataset = FlexDataDistribution.iid_distribution(flex_data, n_clients=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Federating a model with FLEXible\n",
    "\n",
    "Once we've federated the dataset, we have to create the FlexPool. The FlexPool class simulates a real-time scenario for federated learning, so it is in charge of the communications across the actors. The class FlexPool will assign to each actor a role (client, aggregator, server), so they can communicate during the training phase.\n",
    "\n",
    "Please, check the notebook about the actors (TODO: Hacer notebook actores y sus relaciones) to know more about the actors and their relationships in FLEXible.\n",
    "\n",
    "To create a Pool of actors, we need to have a federated dataset, like we've just done, and the model to initialize in the server side, because the server will send the model to the clients so they can train the model. As we have the federated dataset (flex_dataset), we will now create the model.\n",
    "\n",
    "In this case, we will use a model from the tensorflow hub, so we dont have to worry about coding it. We also consider a federated setup commonly know as client server architecture, where a server orchestrates the training of federated clients in every round.\n",
    "\n",
    "In the following, we create a client server architecture and provide a function to initialize the server model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_server_model(flex_model, *args, **kwargs):\n",
    "    print(\"Initializing server model.\")\n",
    "    model = TextClassificationModel(vocab_size=kwargs['vocab_size'],\n",
    "                                    embed_dim=kwargs['embed_dim'],\n",
    "                                    num_class=kwargs['num_class'])\n",
    "    flex_model['model'] = model\n",
    "    flex_model['criterion'] = kwargs['criterion']\n",
    "    flex_model['optimizer'] = kwargs['optimizer'](model.parameters(), lr=kwargs['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab_hf) # Length of the vocab\n",
    "embed_dim = 64 # We set the embed dim to 64 for creating a low model, just for the tutorial\n",
    "num_class = len(set(train_labels)) # Number of classes on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flex.pool import FlexPool\n",
    "\n",
    "flex_pool = FlexPool.client_server_architecture(fed_dataset=flex_dataset, init_func=initialize_server_model,\n",
    "                                                vocab_size=vocab_size, embed_dim=embed_dim,\n",
    "                                                num_class=num_class,\n",
    "                                                criterion=torch.nn.CrossEntropyLoss(),\n",
    "                                                optimizer=torch.optim.SGD,\n",
    "                                                learning_rate=5\n",
    "                                                )\n",
    "clients = flex_pool.clients\n",
    "server = flex_pool.servers\n",
    "print(f\"Server node is indentified by {server.actor_ids}\")\n",
    "print(f\"Client nodes are identified by {clients.actor_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the criterion and the optimizer now, even thought they will be used later. This is needed becaue we may want to use optimizers that saves a state, and that will be used in multiple rounds of the federated learning, and we don't want it to initialize in each round. The SGD optimizer does not keep and state, and it's not needed to be declared here but, this is the best practice so you don't forget it later in the trainin function. \n",
    "\n",
    "We have to create the function that will deploy the model to the clients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_model_to_clients(server_model, clients_model, *args, **kwargs):\n",
    "    print(\"Initializing model at a client\")\n",
    "    for client_id in clients_model:\n",
    "        clients_model[client_id] = deepcopy(server_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work in an easier way, FlexPool let the use to have organized pools, such as clients, aggregators or servers. This helps to understand how we are connecting the actors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients = flex_pool.clients\n",
    "server = flex_pool.servers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply all the primitives, such as the deploy step, we will use the **map** function from *FlexPool*. The map function works in the following way: the pool that calls the function map, is the one that will send a message to the destiny pool. If we don't specify it to any pool, no destiny pool, it will \"send\" the message to the same pool that it's calling the map function. This is needed if we want to tell the clients to train/evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server.map(deploy_model_to_clients, clients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is deployed on the clients, is time to create the training function. Our PyTorch model needs a DataLoader to feed the data into the model in batches. So first, let's the requirements for the DataLoader, that in our case, is the collate function. This collate function will help us to apply the preprocessing for each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, label):\n",
    "        self.text = text\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.text[idx], self.label[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for (_text, _label) in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, we need to define the training function for our model. GIven that, we will create two different functions for training, one for the model, and one for the client. In TensorFlow we can call the *fit* method and the model will just train, but here we have to create this fit method.\n",
    "\n",
    "First we create the fit method as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_client_model(model, dataloader, *args, **kwargs):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "    optimizer = kwargs['optimizer']\n",
    "    criterion = kwargs['criterion']\n",
    "    epoch = kwargs['epoch']\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text, offsets)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                    '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
    "                                                total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the train method for our federated environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(client_model, client_data, *args, **kwargs):\n",
    "    print(\"Training model at client.\")\n",
    "    model = client_model['model']\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=kwargs['learning_rate'])\n",
    "    optimizer = client_model['optimizer']\n",
    "    # criterion = torch.nn.CrossEntropyLoss()\n",
    "    criterion = client_model['criterion']\n",
    "    epochs = kwargs['epochs']\n",
    "    client_dataset = TextDataset(text=client_data.X_data, label=client_data.y_data)\n",
    "    client_dataloader = DataLoader(client_dataset, batch_size=kwargs['batch_size'], shuffle=False, collate_fn=collate_batch)\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        fit_client_model(model=model, dataloader=client_dataloader,\n",
    "                        optimizer=optimizer, criterion=criterion, epoch=epoch)\n",
    "        print('-' * 59)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | '.format(epoch,\n",
    "                                            time.time() - epoch_start_time))\n",
    "        print('-' * 59)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will train the model in the clients side. We will use the *map function* to tell the clients to train the model, and, to do so, we just need to use this function from the clients pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients.map(train, batch_size=512, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trained the model we have to aggregate the weights. To do so, clients will send the weights to the aggregator, and she will perform the aggregation step. For the tutorial, we will implement the FevAvg aggregation mechanism. That is, the aggreation step is split in two steps, 1) for collecting the weights from each client and 2) for averaging them.\n",
    "\n",
    "First, we select the aggregator, which in this case is the same as the server, because in the client server architecture, the server is also an aggregator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregator = flex_pool.aggregators\n",
    "aggregator.actor_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_weights(client_model, aggregator_model, **kwargs):\n",
    "    print(\"Collecting weights.\")\n",
    "    if 'weights' not in aggregator_model[\"server\"]:\n",
    "        aggregator_model[\"server\"]['weights'] = []\n",
    "    client_weights = [param.cpu().data.numpy() for param in client_model['model'].parameters()]\n",
    "    aggregator_model[\"server\"]['weights'].append(client_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients.map(collect_weights, aggregator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_weights(agg_model, *args):\n",
    "    print(\"Aggregating weights\")\n",
    "    averaged_weights = np.mean(np.array(agg_model['weights']), axis=0)\n",
    "    with torch.no_grad():\n",
    "        for old, new in zip(agg_model['model'].parameters(), averaged_weights):\n",
    "            old.data = torch.from_numpy(new).float()\n",
    "    agg_model[\"weights\"] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregator.map(aggregate_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's turn from the server to update the weights from the clients models and then evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_global_model_to_clients(server_model, clients_models, *args, **kwargs):\n",
    "    print(\"Deploying the global model on the clients.\")\n",
    "    aggregated_weights = [param.cpu().data.numpy() for param in server_model['model'].parameters()]\n",
    "    with torch.no_grad():\n",
    "        for client_model in clients_models:\n",
    "            for old, new in zip(clients_models[client_model]['model'].parameters(), aggregated_weights):\n",
    "                old.data = torch.from_numpy(new).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server.map(deploy_global_model_to_clients, clients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we can evaluate the model with the test set that we prepared at the begining of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_torch(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predicted_label = model(text, offsets)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(flex_model, data, *args, **kwargs):\n",
    "    model = flex_model['model']\n",
    "    criterion = flex_model['criterion']\n",
    "    if data is not None:\n",
    "        print(\"Evaluating model at client.\")\n",
    "        client_dataset = TextDataset(data.X_data, data.y_data)\n",
    "        client_dataloader = DataLoader(client_dataset, batch_size=kwargs['batch_size'], shuffle=True, collate_fn=collate_batch)\n",
    "        results_local = evaluate_model_torch(model, client_dataloader, criterion)\n",
    "        print(f\"Results at client on client's data: {results_local}\")\n",
    "    else:\n",
    "        print(\"Evaluating model at server\")\n",
    "    test_dataset = TextDataset(kwargs['test_examples'], kwargs['test_labels'])\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=kwargs['batch_size'], shuffle=True, collate_fn=collate_batch)\n",
    "    results = evaluate_model_torch(model, test_dataloader, criterion)\n",
    "    print(f\"Results on test data: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server.map(evaluate_model, test_examples=test_examples, test_labels=test_labels, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients.map(evaluate_model, test_examples=test_examples, test_labels=test_labels, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together\n",
    "\n",
    "You just have trained a model for 1 round using FLEXible. Now, you could set up all together in a function and iterate for multiple rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_n_rounds(n_rounds, batch_size, epochs):\n",
    "    pool = FlexPool.client_server_architecture(fed_dataset=flex_dataset, init_func=initialize_server_model,\n",
    "                                                vocab_size=vocab_size, embed_dim=embed_dim,\n",
    "                                                num_class=num_class,\n",
    "                                                criterion=torch.nn.CrossEntropyLoss(),\n",
    "                                                optimizer=torch.optim.SGD,\n",
    "                                                learning_rate=5\n",
    "                                                )\n",
    "    pool.servers.map(deploy_model_to_clients, pool.clients)\n",
    "    for i in range(n_rounds):\n",
    "        print(f\"\\nRunning round: {i}\\n\")\n",
    "        pool.clients.map(train, batch_size=batch_size, epochs=epochs)\n",
    "        pool.clients.map(evaluate_model, test_examples=test_examples, test_labels=test_labels, batch_size=8)\n",
    "        pool.clients.map(collect_weights, pool.aggregators)\n",
    "        pool.aggregators.map(aggregate_weights)\n",
    "        pool.servers.map(deploy_global_model_to_clients, pool.clients)\n",
    "        pool.servers.map(evaluate_model, test_examples=test_examples, test_labels=test_labels, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_n_rounds(n_rounds=4, batch_size=512, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END\n",
    "Congratulations, now you know how to train a model using FLEXible for multiples rounds. Remember that it's important to first deploy/initialize the model on the clients, so you can run the rounds without problem!"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c16e99a8b049a3c2333046a7199861cad81dc55b88bad19d31a6edddeb39a963"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('flex')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
