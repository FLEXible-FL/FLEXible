{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example os use of FLEXible to train a LSTM for Text Generation with a custom Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Federate Dataset\n",
    "\n",
    "In the first section we're going to federate our dataset. For this tutorial, we use Reddit clean jokes dataset to train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from flex.data import FlexDataset, FlexDataObject, FlexDatasetConfig, FlexDataDistribution\n",
    "from flex.pool import FlexPool\n",
    "\n",
    "from utils import print_function, add_torch_dataset_to_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we're going to load our data from the CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv('reddit-cleanjokes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['Joke'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before federating the dataset, it's necessary to create the vocabulary. The vocabulary will be a global across clients, as we can simulate that clients will use the same pre-trained embeddings such as GloVe or FastText. For using those embeddings we will use the torchtext library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab.vectors import FastText\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext = FastText(language=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "def yield_sentences(text):\n",
    "    for sentence in text:\n",
    "        yield tokenizer(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_iterator(iterator=yield_sentences(X), min_freq=2)\n",
    "vocab.append_token('UNK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the vocab, we're going to load the Fasttext embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = fasttext.get_vecs_by_tokens([\"Hi\", \"how\", \"are\", \"you\"], lower_case_backup=True)\n",
    "# ret = fasttext.get_vecs_by_tokens(\"Hi, how are you\", lower_case_backup=True)\n",
    "fasttext.vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create the Dataset class that will use each client to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, tokenizer, itos, stoi, sequence_len):\n",
    "        text = ' '.join([sentence[0] for sentence in text])\n",
    "        # self.text = ' '.join([tokenizer(sentence[0]) for sentence in text]))\n",
    "        self.text = tokenizer(text)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_itos = itos\n",
    "        self.vocab_stoi = stoi\n",
    "        self.sequence_length = sequence_len\n",
    "        # self.words_indexes = [[stoi.get(word, stoi['UNK']) for word in sentence] for sentence in text]\n",
    "        self.words_indexes = [stoi.get(word, stoi.get('UNK')) for word in self.text]\n",
    "\n",
    "    def __len__(self):\n",
    "        # return len(self.text)\n",
    "        return len(self.words_indexes) - self.sequence_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            torch.tensor(self.words_indexes[index:index+self.sequence_length]),\n",
    "            torch.tensor(self.words_indexes[index+1:index+self.sequence_length+1]),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.get_stoi().get('hearts', vocab.get_stoi().get('UNK'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------\n",
    "Stop until bug in FlexDataDistribution.from_cofig() it's solved. \n",
    "\n",
    "Bug: Can't federate if the array has only one feature\n",
    "\n",
    "Temporal solution: Change in flex_data_distribution in function __sample_with_weights, I added at line 177:\n",
    "sub_features_indices = None # slice(\n",
    "        #     None\n",
    "        # )  # Default slice for features, it includes all the features\n",
    "\n",
    "Remove that 3 lines to back to normal.\n",
    "\n",
    "--------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdata = FlexDataObject(X)\n",
    "config = FlexDatasetConfig(seed=0)\n",
    "config.n_clients = 2\n",
    "config.replacement = False # ensure that clients do not share any data\n",
    "config.client_names = ['client1', 'client2']\n",
    "# config.weights = [0.2] * config.n_clients # each client has only 20% of its assigned class\n",
    "config.weights = None\n",
    "fld = FlexDataDistribution.from_config(cdata=cdata, config=config)\n",
    "#fld = FlexDataDistribution.iid_distribution(cdata=cdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make each client have the Dataset, we have to use the map function from the FlexDataset class. With the map function, we will make the client have a Dataset internally, so we can train the model for each client with it's own data. \n",
    "\n",
    "The map function recieve a function to apply to each client, so now we create the function that we want to apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_torch_dataset_to_client_2(client, *args, **kwargs):\n",
    "    \"\"\"Function to create a dataset for each client. We keep the \n",
    "    X_data property as we don't want to change the raw text, but\n",
    "    it should be changed for less memory usage.\n",
    "\n",
    "    Args:\n",
    "        client (FlexDataObject): Client to create a TextDataset\n",
    "\n",
    "    Returns:\n",
    "        FlexDataObject: Client with a TextDataset in her data\n",
    "    \"\"\"\n",
    "    new_client = deepcopy(client)\n",
    "    new_client_dataset = TextDataset(new_client.X_data, kwargs['tokenizer'], \n",
    "                                        kwargs['itos'], kwargs['stoi'], kwargs['sequence_len'])\n",
    "    new_client.dataset = new_client_dataset\n",
    "\n",
    "    return new_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = fld['client1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(client.X_data)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_client = add_torch_dataset_to_client_2(fld['client1'], tokenizer=tokenizer, itos=vocab.get_itos(), stoi=vocab.get_stoi(), sequence_len=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_client.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_fld = fld.map(num_proc=2, func=add_torch_dataset_to_client, tokenizer=tokenizer,\n",
    "                                                                itos=vocab.get_itos(),\n",
    "                                                                stoi=vocab.get_stoi(),\n",
    "                                                                sequence_len=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_fld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_federated_dataset = FlexDataset({\n",
    "    client: add_torch_dataset_to_client(fld[client], tokenizer=tokenizer, itos=vocab.get_itos(),\n",
    "                                stoi=vocab.get_stoi(), sequence_len=100) \n",
    "    for client in fld\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Create the architecture\n",
    "\n",
    "\n",
    "Once we've federated our dataset, it's time to create the federated environment. In this case, we will use the FlexPool class to create the actors. The FlexPool class simulates a real-time scenario for federated learning, so we have to create each actor and it's role during the creationg and training of the model.\n",
    "\n",
    "To initialize the Pool of actors we need a federated dataset, the *fld* variable we've created. We can use the constructor or use the functions given to create a fixed architecture. In this tutorial we will use a client-server architecture, so we will use the function client_server_architecture from the FlexPool class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = FlexPool.client_server_architecture(fed_dataset=new_fld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have created a pool of actors that is composed of:\n",
    "- Clients: The clients have the client-role and can access the data of the FlexDataset if they have the same ID.\n",
    "- Server-aggregator: The client-server architecture adds a new actor that has the role of the server, so it can orchestate the training phase, and the aggregator, so it can aggregate the weights.\n",
    "\n",
    "The pool of actors has some communication restrictions, as indicated in the documentation, so to make it easy to understand how the pool works, we can separate actors in different pools based on the role. In our case, we can get two subpools, one with the clients, and one with the server (that also acts as aggregator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients = pool.clients\n",
    "server = pool.servers\n",
    "# Lets take a look at the two pools we've just created.\n",
    "print(f\"Pool of clients: {clients._actors}\")\n",
    "print(f\"Pool of server-aggregator: {server._actors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it's shown in the above cell, the server has two roles, the server one and the aggregator, so she can acts a aggregator too.\n",
    "\n",
    "Now the we have the pools with the actors, we can start the training phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Set up the training round"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training phase has 4 phases:\n",
    "- Initialize/Deploy model\n",
    "- Train model\n",
    "- Aggregate weights\n",
    "- Evaluate model\n",
    "\n",
    "This functions aren't available in the FlexPool class, as they will be different for each model, so the user must create the functions and apply them to the actors using the *map* function from FlexPool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Init/deploy models\n",
    "\n",
    "The fist step to init the training phase is to deplay the model across the clients that will train the model. In this example we only have two clients, so we are going to use both clients to train the model.\n",
    "\n",
    "Once we've federated our dataset, it's time to create the model to train. Here we will use a simple LSTM model.\n",
    "\n",
    "The model will have a layer of embeddings generated from it's own vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_vocab):\n",
    "        super(Model, self).__init__()\n",
    "        self.lstm_size = 128\n",
    "        self.embedding_dim = 128\n",
    "        self.num_layers = 3\n",
    "\n",
    "        # n_vocab = len(dataset.uniq_words)\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=n_vocab,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.lstm_size,\n",
    "            hidden_size=self.lstm_size,\n",
    "            num_layers=self.num_layers,\n",
    "            dropout=0.2,\n",
    "        )\n",
    "        self.fc = nn.Linear(self.lstm_size, n_vocab)\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.embedding(x)\n",
    "        output, state = self.lstm(embed, prev_state)\n",
    "        logits = self.fc(output)\n",
    "        return logits, state\n",
    "\n",
    "    def init_state(self, sequence_length):\n",
    "        return (torch.zeros(self.num_layers, sequence_length, self.lstm_size),\n",
    "                torch.zeros(self.num_layers, sequence_length, self.lstm_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is defined, we have to define the function that will initialize the model for each client that participate in the training phase. After defining this function, we can use the function *map* from FlexPool, to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server._models = {serv: Model(n_vocab=len(vocab)) for serv in server._actors}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(server_model, clients_models, *args, **kwargs):\n",
    "    for client_model in clients_models:\n",
    "        clients_models[client_model] = deepcopy(server_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server.map(initialize_model, clients) # As we have only one server, we take the first model only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients._models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have initilized the model for each client, and we have to train the model. To train the model we have to prepare the train function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, model, *args, **kwargs):\n",
    "    # Create the torch dataset for the client\n",
    "    client_dataset = TextDataset(data.X_data, kwargs['tokenizer'], \n",
    "                                kwargs['itos'], kwargs['stoi'], kwargs['sequence_len'])\n",
    "    # Set the model to train\n",
    "    model.train()\n",
    "    # Create the DataLoader, loss function and optimizer\n",
    "    dataloader = DataLoader(client_dataset, batch_size=kwargs['batch_size'])\n",
    "    criterion = nn.CrossEntropyLoss() # kwargs['criterion']\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001) # kwargs['optimizer']\n",
    "    # Loop the epochs to train the model\n",
    "    for epoch in range(kwargs['epochs']):\n",
    "        # Get the initial state\n",
    "        state_h, state_c = model.init_state(data.dataset.sequence_length)\n",
    "        # Iterate across the batches\n",
    "        for batch, (x, y) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "            loss = criterion(y_pred.transpose(1,2), y)\n",
    "\n",
    "            state_h = state_h.detach()\n",
    "            state_c = state_c.detach()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            print({ 'epoch': epoch, 'batch': batch, 'loss': loss.item() })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients.map(train, batch_size=256, epochs=10, tokenizer=tokenizer, itos=vocab.get_itos(), stoi=vocab.get_stoi(), sequence_len=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have traint all the models available for training, so it's time to aggregate them. We have to create a function and then apply it with the *map* from FlexPool. In this case, the source pool will be the clients, and the destiny pool the aggregator. \n",
    "\n",
    "To get the weights of a model on pytorch, we can use model.parameters(), or model.named_parameters() if we want to get the layer's name too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(orig_models, dst_model, *args, **kwargs):\n",
    "    \"\"\"Function that aggregate the weights\n",
    "\n",
    "    Args:\n",
    "        orig_models (nn.Module): Original model traint\n",
    "        dst_model (nn.Module): Destiny model (aggregator model)\n",
    "    \"\"\"\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c16e99a8b049a3c2333046a7199861cad81dc55b88bad19d31a6edddeb39a963"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('flex')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
