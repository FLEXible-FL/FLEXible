{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vertical Federated Learning with the Adult-income dataset\n",
    "\n",
    "In this notebook we show how to use FLEXible to simulate a Vertical Federated Learning (VFL) scenario with a neural network using Pytorch. We implement the VFL process described in paper: [Vertical Federated Learning: Challenges, Methodologies and Experiments](https://arxiv.org/abs/2202.04309)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we download the raw dataset using the `ucimlrepo` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flex.model import FlexModel\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# fetch dataset\n",
    "adult = fetch_ucirepo(id=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We preprocess it, making the targets integers and removing rows with NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# data (as pandas dataframes)\n",
    "x_cols = adult.data.features.columns.tolist()\n",
    "y_cols = adult.data.targets.columns.tolist()\n",
    "adult = pd.concat([adult.data.features, adult.data.targets], axis=1)\n",
    "adult = adult.dropna()\n",
    "x_data = adult[x_cols]\n",
    "y_data = adult[y_cols]\n",
    "# Transform string labels \">50K\", \"<=50K\" to integer labels 1, 0\n",
    "y_data = y_data[\"income\"].apply(lambda label: \">\" in label).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a `Dataset` object using the preprocessed dataset and a `FedDatasetConfig` object showing how we want to simulate the vertical split: \n",
    "\n",
    "There are three nodes, one `host` and two `guest`, only the former keeps the labels and each one has 4 features. As we are performing a vertical split, we need to provide weights equal to one, `replacement=False` and `shuffle=False`, to ensure that each node gets the same data points but different features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flex.data import Dataset, FedDatasetConfig, FedDataDistribution\n",
    "\n",
    "dataset = Dataset.from_numpy(x_data.to_numpy(), y_data.to_numpy())\n",
    "\n",
    "config = FedDatasetConfig(\n",
    "    seed=0,\n",
    "    n_nodes=3,\n",
    "    node_ids=[\"host\", \"guest_1\", \"guest_2\"],\n",
    "    shuffle=False,\n",
    "    replacement=True,\n",
    "    keep_labels=[True, False, False],\n",
    "    weights=[\n",
    "        1.0,\n",
    "        1.0,\n",
    "        1.0,\n",
    "    ],  # Ensure that each node gets the entire dataset, not a subset\n",
    "    features_per_node=[\n",
    "        [0, 1, 2, 3],  # ['age', 'workclass', 'fnlwgt', 'education'] for \"host\" node\n",
    "        [\n",
    "            4,\n",
    "            5,\n",
    "            6,\n",
    "            7,\n",
    "        ],  # ['education-num', 'marital-status', 'occupation', 'relationship'] for \"guest_1\" node\n",
    "        [\n",
    "            8,\n",
    "            9,\n",
    "            10,\n",
    "            11,\n",
    "            12,\n",
    "            13,\n",
    "        ],  # ['race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country'] for \"guest_2\" node\n",
    "    ],\n",
    ")\n",
    "\n",
    "fed_dataset = FedDataDistribution.from_config(dataset, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have split the data among nodes, we one hot encode each local dataset. Beware, only the \"host\" node has labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def preprocess_x_data(local_dataset: Dataset):\n",
    "    ohe_encoder = preprocessing.OneHotEncoder(\n",
    "        sparse_output=False, handle_unknown=\"ignore\"\n",
    "    )\n",
    "    x_data = local_dataset.X_data.to_numpy()\n",
    "    y_data = local_dataset.y_data\n",
    "    encoded_x_data = ohe_encoder.fit_transform(x_data)\n",
    "    return Dataset.from_numpy(encoded_x_data, y_data)\n",
    "\n",
    "\n",
    "# One hot encode each dataset individually\n",
    "fed_dataset = fed_dataset.apply(preprocess_x_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once data is federated, we assign a `FlexRole` to each `node_id`, to create a `FlexPool` which simulates the Vertical Federated Learning flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flex.actors import FlexRole, FlexActors\n",
    "\n",
    "actors = FlexActors()\n",
    "actors[\"host\"] = FlexRole.server_aggregator_client\n",
    "actors[\"guest_1\"] = FlexRole.client\n",
    "actors[\"guest_2\"] = FlexRole.client\n",
    "\n",
    "from flex.pool import FlexPool\n",
    "\n",
    "pool = FlexPool(fed_dataset, actors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VFL model in this simulation is a simple fully-connected network with three layers of 48, 96 and 196 hidden units. The bottom model has 48 hidden units and its output size is 32. The top model has 196 hidden units, its input size is 32*3=96 and it outputs the final predictions of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_units, out_dim):\n",
    "        super().__init__()\n",
    "        self.input_layer = torch.nn.Linear(input_dim, hidden_units)\n",
    "        self.output_layer = torch.nn.Linear(hidden_units, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each node owns the bottom model, which has 48 hidden units and its output dimension is 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_bottom_model(local_model: FlexModel, local_dataset: Dataset):\n",
    "    input_dim = local_dataset.X_data.to_numpy().shape[-1]\n",
    "    hidden_units = 48\n",
    "    out_dim = 32\n",
    "    local_model[\"bottom_model\"] = MLP(input_dim, hidden_units, out_dim)\n",
    "    local_model[\"bottom_optimizer\"] = torch.optim.Adam(\n",
    "        local_model[\"bottom_model\"].parameters(), lr=0.0002, weight_decay=0.01\n",
    "    )\n",
    "\n",
    "\n",
    "pool.map(initialize_bottom_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The host node also has the top part of the model, which receives as input the outputs of the bottom model, this its input dimension is 32*3=96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_top_model(local_model: FlexModel, local_dataset: Dataset):\n",
    "    input_dim = 32 * 3\n",
    "    hidden_units = 196\n",
    "    out_dim = 1\n",
    "    local_model[\"loss\"] = torch.nn.BCELoss()\n",
    "    local_model[\"top_model\"] = MLP(input_dim, hidden_units, out_dim)\n",
    "    local_model[\"top_optimizer\"] = torch.optim.Adam(\n",
    "        local_model[\"top_model\"].parameters(), lr=0.0002, weight_decay=0.01\n",
    "    )\n",
    "\n",
    "\n",
    "# The host has both the bottom and the top model\n",
    "host_pool = pool.select(lambda node_id, role: node_id == \"host\")\n",
    "host_pool.map(initialize_top_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions implement the logic of a FL round:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_batch_sampler(local_model: FlexModel, local_dataset: Dataset, seed=0):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from torch.utils.data import DataLoader, RandomSampler\n",
    "\n",
    "    common_random_state = 1001\n",
    "    test_size = 0.2\n",
    "    batch_size = 500\n",
    "    # Perform train-test split here, in order to generate\n",
    "    if local_model.actor_id == \"host\":\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            local_dataset.X_data,\n",
    "            local_dataset.y_data,\n",
    "            test_size=test_size,\n",
    "            random_state=common_random_state,\n",
    "        )\n",
    "        train_dataset = Dataset.from_list(X_train, y_train)\n",
    "        test_dataset = Dataset.from_list(X_test, y_test)\n",
    "    else:\n",
    "        train_dataset, test_dataset = train_test_split(\n",
    "            local_dataset.X_data, test_size=test_size, random_state=common_random_state\n",
    "        )\n",
    "    # Same seed for all RandomSamplers\n",
    "    generator = torch.Generator()\n",
    "    generator = generator.manual_seed(seed)\n",
    "    train_sampler = RandomSampler(train_dataset, replacement=False, generator=generator)\n",
    "    local_model[\"train_batch_sampler\"] = iter(\n",
    "        DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "    )\n",
    "    local_model[\"test_batch_sampler\"] = iter(\n",
    "        DataLoader(test_dataset, batch_size=len(test_dataset))\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_bottom_outputs(local_model: FlexModel, local_dataset: Dataset):\n",
    "    bottom_model = local_model[\"bottom_model\"]\n",
    "    try:\n",
    "        data = next(local_model[\"train_batch_sampler\"])\n",
    "        if local_model.actor_id == \"host\":\n",
    "            batch_data, local_model[\"y_batch\"] = data\n",
    "        else:\n",
    "            batch_data = data\n",
    "        local_model[\"bottom_optimizer\"].zero_grad()\n",
    "        local_model[\"bottom_output\"] = bottom_model(batch_data.float())\n",
    "        return True\n",
    "    except StopIteration:\n",
    "        return False\n",
    "\n",
    "\n",
    "def forward_bottom_outputs(host_model: FlexModel, models):\n",
    "    bottom_outputs = []\n",
    "    for k in models:\n",
    "        bottom_outputs.append(models[k][\"bottom_output\"])\n",
    "    bottom_outputs = torch.cat(bottom_outputs, axis=1)\n",
    "    host_model[\"top_optimizer\"].zero_grad()\n",
    "    top_outputs = host_model[\"top_model\"](bottom_outputs)\n",
    "    logits = torch.sigmoid(top_outputs)\n",
    "    host_model[\"logits\"] = logits.squeeze(axis=1)\n",
    "\n",
    "\n",
    "def backward_pass_top(local_model: FlexModel, local_dataset: Dataset):\n",
    "    logits = local_model[\"logits\"]\n",
    "    loss = local_model[\"loss\"](logits, local_model[\"y_batch\"].float())\n",
    "    loss.backward()\n",
    "    local_model[\"top_optimizer\"].step()\n",
    "\n",
    "\n",
    "def backward_pass_bottom(local_model: FlexModel, local_dataset: Dataset):\n",
    "    local_model[\"bottom_optimizer\"].step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above-defined functions an FL round is as follows:\n",
    "\n",
    "- First, all nodes agree to iterate their datasets in such a way that their features align using `setup_batch_sampler`\n",
    "- Then, while possible each node generates the bottom output of its data using `generate_bottom_outputs`\n",
    "- The bottom outputs are gathered by the host and feeded to the top model `forward_bottom_outputs`\n",
    "- Now, the backward pass begins, first the top model is updated using `backward_pass_top`, then the bottom models are updated using `backward_pass_bottom`.\n",
    "- This process is repeated until the nodes have iterated their full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool.map(setup_batch_sampler)\n",
    "\n",
    "while all(pool.map(generate_bottom_outputs)):\n",
    "    host_pool.map(forward_bottom_outputs, pool)\n",
    "    host_pool.map(backward_pass_top)\n",
    "    pool.map(backward_pass_bottom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still, we are missing the code to evaluate the performance of the whole model, which is as follows:\n",
    "\n",
    "1. Generate the bottom outputs of the test set\n",
    "1. The host gathers the bottom outputs, feeds them to the top model and computes the scores. Note that, the host node is the only node with labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_bottom_outputs(local_model: FlexModel, local_dataset: Dataset):\n",
    "    with torch.no_grad():\n",
    "        bottom_model = local_model[\"bottom_model\"]\n",
    "        data = next(local_model[\"test_batch_sampler\"])\n",
    "        if local_model.actor_id == \"host\":\n",
    "            batch_data, local_model[\"test_y\"] = data\n",
    "        else:\n",
    "            batch_data = data\n",
    "        local_model[\"test_bottom_output\"] = bottom_model(batch_data.float())\n",
    "\n",
    "\n",
    "def evalute_test_bottom_outputs(host_model: FlexModel, models):\n",
    "    from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "    with torch.no_grad():\n",
    "        bottom_outputs = []\n",
    "        for k in models:\n",
    "            bottom_outputs.append(models[k][\"test_bottom_output\"])\n",
    "        bottom_outputs = torch.cat(bottom_outputs, axis=1)\n",
    "        top_outputs = host_model[\"top_model\"](bottom_outputs)\n",
    "        logits = torch.sigmoid(top_outputs)\n",
    "        logits = logits.squeeze(axis=1)\n",
    "        acc = accuracy_score(host_model[\"test_y\"], (logits >= 0.5).long())\n",
    "        auc = roc_auc_score(host_model[\"test_y\"], logits)\n",
    "        print(f\"test auc: {auc:.4f}, test acc: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consequentially, a full VFL simulation, where every 5 rounds the whole model is evaluated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_rounds = 100\n",
    "\n",
    "for i in range(fl_rounds):\n",
    "    globa_seed = int(torch.empty((), dtype=torch.int32).random_().item())\n",
    "    pool.map(setup_batch_sampler, seed=globa_seed)\n",
    "    while all(pool.map(generate_bottom_outputs)):  # Train\n",
    "        host_pool.map(forward_bottom_outputs, pool)\n",
    "        host_pool.map(backward_pass_top)\n",
    "        pool.map(backward_pass_bottom)\n",
    "    if (i + 1) % 5 == 0:  # Evaluate\n",
    "        print(f\"FL Round {i+1}\", end=\" \")\n",
    "        pool.map(generate_test_bottom_outputs)\n",
    "        host_pool.map(evalute_test_bottom_outputs, pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
