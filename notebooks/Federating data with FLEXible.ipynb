{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to federate datasets using FLEXible\n",
    "\n",
    "In this notebooks, we show a few of the many ways in which FLEXible can federate a centralized dataset. We will use MNIST and CIFAR10 datasets in this notebooks\n",
    "\n",
    "First, we download it and split it in train and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "ds_train, ds_test = tfds.load(\n",
    "    \"mnist\",\n",
    "    split=[\"train\", \"test\"],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    batch_size=-1,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use our tools, we need to encapsulate the dataset in a `Dataset`. \n",
    "\n",
    "Note that train_X and train_y are assumed to be NumPy arrays and train_y must be a one dimensional NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flex.data import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_tfds_image_dataset(ds_train)\n",
    "test_dataset = Dataset.from_tfds_image_dataset(ds_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To federate a centralized dataset, it is required to describe the federation process in a `FedDatasetConfig` object.\n",
    "The following table shows the compatiblity of each option:\n",
    "\n",
    "| Options compatibility   | **n_nodes** | **node_ids** | **weights** | **weights_per_label** | **replacement** | **labels_per_node** | **features_per_node** | **indexes_per_node** | **group_by_label_index** | **keep_labels** | **shuffle** |\n",
    "|-------------------------|-------------|--------------|-------------|-----------------------|-----------------|---------------------|-----------------------|----------------------|--------------------------|-----------------|-------------|\n",
    "| **n_nodes**             | -           | Y            | Y           | Y                     | Y               | Y                   | Y                     | N                    | N                        | Y               | Y           |\n",
    "| **node_ids**            | -           | -            | Y           | Y                     | Y               | Y                   | Y                     | Y                    | N                        | Y               | Y           |\n",
    "| **weights**             | -           | -            | -           | N                     | Y               | Y                   | Y                     | N                    | N                        | Y               | Y           |\n",
    "| **weights_per_label**   | -           | -            | -           | -                     | Y               | N                   | N                     | N                    | N                        | Y               | Y           |\n",
    "| **replacement**         | -           | -            | -           | -                     | -               | Y                   | N                     | N                    | N                        | Y               | Y           |\n",
    "| **labels_per_node**     | -           | -            | -           | -                     | -               | -                   | N                     | N                    | N                        | Y               | Y           |\n",
    "| **features_per_node**   | -           | -            | -           | -                     | -               | -                   | -                     | N                    | N                        | Y               | Y           |\n",
    "| **indexes_per_node**    | -           | -            | -           | -                     | -               | -                   | -                     | -                    | N                        | Y               | Y           |\n",
    "| **group_by_label_index**| -           | -            | -           | -                     | -               | -                   | -                     | -                    | -                        | N               | Y           |\n",
    "| **keep_labels**         | -           | -            | -           | -                     | -               | -                   | -                     | -                    | -                        | -               | Y           |\n",
    "| **shuffle**             | -           | -            | -           | -                     | -               | -                   | -                     | -                    | -                        | -               | -           |\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "- seed: Optional[int]\n",
    "    Seed used to make the federated dataset generated reproducible with this configuration. Default None.\n",
    "- n_nodes: int\n",
    "    Number of nodes among which to split a centralized dataset. Default 2.\n",
    "- shuffle: bool\n",
    "    If True data is shuffled before being sampled. Default False.\n",
    "- node_ids: Optional[List[Hashable]]\n",
    "    Ids to identifty each node, if not provided, nodes will be indexed using integers. If n_nodes is also \\\n",
    "    given, we consider up to n_nodes elements. Default None.\n",
    "- weights: Optional[npt.NDArray]\n",
    "    A numpy.array which provides the proportion of data to give to each node. Default None.\n",
    "- weights_per_label: Optional[npt.NDArray]\n",
    "    A numpy.array which provides the proportion of data to give to each node and class of the dataset to federate. \\\n",
    "    We expect a bidimensional array of shape (n, m) where \"n\" is the number of nodes and \"m\" is the number of labels of \\\n",
    "    the dataset to federate. Default None.\n",
    "- replacement: bool\n",
    "    Whether the samping procedure used to split a centralized dataset is with replacement or not. Default False\n",
    "- labels_per_node: Optional[Union[int, npt.NDArray, Tuple[int]]]\n",
    "    labels to assign to each node, if provided as an int, it is the number labels per node, if provided as a \\\n",
    "    tuple of ints, it establishes a mininum and a maximum of number of labels per node, a random number sampled \\\n",
    "    in such interval decides the number of labels of each node. If provided as a list of lists, it establishes the labels \\\n",
    "    assigned to each node. Default None.\n",
    "- features_per_node: Optional[Union[int, npt.NDArray, Tuple[int]]]\n",
    "    Features to assign to each node, it share the same interface as labels_per_node. Default None.\n",
    "- indexes_per_node: Optional[npt.NDArray]\n",
    "    Data indexes to assign to each node. Default None.\n",
    "- group_by_label_index: Optional[int]\n",
    "    Index which indicates which feature unique values will be used to generate federated nodes. Default None.\n",
    "- keep_labels: Optional[list[bool]]\n",
    "    Whether each node keeps or not the labels or y_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement the following description:\n",
    "\n",
    "We have 10 federated clients, that do not share any instances, each client with data from a single class and with a 20% of the total data available for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flex.data import FedDatasetConfig\n",
    "import numpy as np\n",
    "\n",
    "config = FedDatasetConfig(seed=0)  # We fix a seed to make our federation reproducible\n",
    "config.n_nodes = 10  # 10 clients\n",
    "config.replacement = False  # ensure that clients do not share any data\n",
    "config.labels_per_node = [\n",
    "    [i] for i in np.unique(train_dataset.y_data)\n",
    "]  # assign each client one class\n",
    "config.weights = [\n",
    "    0.2\n",
    "] * config.n_nodes  # each client has only 20% of its assigned class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(train_dataset.y_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the generated `FedDatasetConfig` to a `Dataset`, which encapsulates the centralized dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flex.data import FedDataDistribution\n",
    "\n",
    "federated_dataset = FedDataDistribution.from_config(\n",
    "    centralized_data=train_dataset, config=config\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the federated data, to confirm that the federated split is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for client in federated_dataset:\n",
    "    print(\n",
    "        f\"Node {client} has class {np.unique(federated_dataset[client].y_data)} and {len(federated_dataset[client])} elements, a sample of them is:\"\n",
    "    )\n",
    "    # pyplot.figure(figsize = (1,10))\n",
    "    fig, ax = plt.subplots(1, 10)  # rows, cols\n",
    "    for i, (x, y) in enumerate(federated_dataset[client]):\n",
    "        ax[i].axis(\"off\")\n",
    "        ax[i].imshow(x, cmap=plt.get_cmap(\"gray\"))\n",
    "        if i >= 9:\n",
    "            break\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federate a dataset using weights to distribute data following a certain distribution\n",
    "\n",
    "We try a more special configuration, we want to federate the dataset such that the number of data per client follows a gaussian distribution, consequently, we need to specify weights from a normal distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_nodes = 500\n",
    "mu, sigma = 100, 1  # mean and standard deviation\n",
    "normal_weights = np.random.default_rng(seed=0).normal(\n",
    "    mu, sigma, n_nodes\n",
    ")  # sample random numbers\n",
    "normal_weights = np.clip(\n",
    "    normal_weights, a_min=0, a_max=np.inf\n",
    ")  # remove negative values\n",
    "normal_weights = normal_weights / sum(normal_weights)  # normalize to sum 1\n",
    "\n",
    "plt.hist(normal_weights, bins=15)\n",
    "plt.title(\"Histogram of normal weights\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = FedDatasetConfig(\n",
    "    seed=0, n_nodes=n_nodes, replacement=False, weights=normal_weights\n",
    ")\n",
    "\n",
    "normal_federated_dataset = FedDataDistribution.from_config(\n",
    "    centralized_data=train_dataset, config=config\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot histogram of data per client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasizes_per_client = [\n",
    "    len(normal_federated_dataset[client]) for client in normal_federated_dataset\n",
    "]\n",
    "n, bins, patches = plt.hist(datasizes_per_client)\n",
    "plt.ylabel(\"Data sizes\")\n",
    "plt.title(\"Histogram of data sizes per client\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A more complex dataset federation\n",
    "\n",
    "Now, lets federate CIFAR10 that fits the following description from [Personalized Federated Learning using Hypernetworks](https://paperswithcode.com/paper/personalized-federated-learning-using)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we sample two/ten classes for each client for CIFAR10/CIFAR100; Next, for each client i and selected class c, we sample $ \\alpha_{i,c} \\sim U(.4, .6)$, and assign it with $\\frac{\\alpha_{i,c}}{\\sum_j \\alpha_{j,c}}$ of the samples for this class. We repeat the above using 10, 50 and 100 clients. This procedure produces clients with different number of samples and classes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) We download the cifar10 dataset using torchivision and create a Dataset with it using ``from_torchvision_dataset``. Note that, it is mandatory to at least provide the ``ToTensor`` transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from flex.data import Dataset\n",
    "\n",
    "cifar10 = datasets.CIFAR10(root=\".\", train=True, download=True, transform=None)\n",
    "cifar10_dataset = Dataset.from_torchvision_dataset(cifar10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Create a ``FedDatasetConfig`` that fits the description given above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flex.data import FedDatasetConfig\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "config = FedDatasetConfig(seed=0)\n",
    "config.replacement = True  # it is not clear whether clients share their data or not\n",
    "config.shuffle = True\n",
    "config.n_nodes = 10\n",
    "num_classes = 10\n",
    "\n",
    "# Assign a sample proportion for each client-class pair\n",
    "alphas = np.random.uniform(0.4, 0.6, [config.n_nodes, num_classes])\n",
    "alphas = alphas / np.sum(alphas, axis=0)\n",
    "config.weights_per_label = alphas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Create the federated dataset by applying the created ``FedDatasetConfig`` to a ``Dataset`` using ``FedDataDistribution.from_config``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flex.data import FedDataDistribution\n",
    "\n",
    "personalized_cifar_dataset = FedDataDistribution.from_config(\n",
    "    centralized_data=cifar10_dataset, config=config\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) (Optional) Check that the data is federated as expected\n",
    "\n",
    "In the following, we show that the proportion of each client data and label are roughtly the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_classes = np.sort(np.unique(cifar10_dataset.y_data))\n",
    "\n",
    "for i in range(config.n_nodes):\n",
    "    client_key = i  # Autogenerated keys are created as numbers 0...n_nodes\n",
    "    for j, cifar_class in enumerate(unique_classes):\n",
    "        indexes = (\n",
    "            personalized_cifar_dataset[client_key].y_data.to_numpy() == cifar_class\n",
    "        )\n",
    "        number_of_elements_per_class = sum(indexes)\n",
    "        if number_of_elements_per_class != 0:\n",
    "            total_elements_per_class = np.sum(\n",
    "                cifar10_dataset.y_data.to_numpy() == cifar_class\n",
    "            )\n",
    "            print(\n",
    "                f\"client id: {client_key}, class {cifar_class}: actual proportion\",\n",
    "                number_of_elements_per_class / total_elements_per_class,\n",
    "                \"vs. expected proportion:\",\n",
    "                config.weights_per_label[i][j],\n",
    "            )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "If we want, we can normalize the dataset of each client easily, using the `apply` function from `FedDataset`, for example we force each client to keep only pair labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng(seed=0)\n",
    "\n",
    "\n",
    "def keep_given_labels(client_dataset: Dataset, selected_labels=None):\n",
    "    if not selected_labels:\n",
    "        selected_labels = []\n",
    "    X, y = client_dataset.to_numpy(x_dtype=object)\n",
    "    indexes = np.isin(y, selected_labels)\n",
    "    X_data = X[indexes]\n",
    "    y_data = y[indexes]\n",
    "    return Dataset.from_array(X_data, y_data)\n",
    "\n",
    "\n",
    "randomly_transformed_federated_dataset = personalized_cifar_dataset.apply(\n",
    "    func=keep_given_labels,  # function to apply\n",
    "    num_proc=1,\n",
    "    selected_labels=[0, 2, 4, 6, 8],  # argument for function\n",
    ")\n",
    "\n",
    "for i, client in enumerate(randomly_transformed_federated_dataset):\n",
    "    print(\n",
    "        f\"Client {client} has classes {np.unique(randomly_transformed_federated_dataset[client].y_data)} and {len(randomly_transformed_federated_dataset[client])} elements, a sample of them is:\"\n",
    "    )\n",
    "    fig, ax = plt.subplots(1, 10)  # rows, cols\n",
    "    for j, (x, y) in enumerate(randomly_transformed_federated_dataset[client]):\n",
    "        ax[j].axis(\"off\")\n",
    "        ax[j].imshow(x, cmap=plt.get_cmap(\"gray\"))\n",
    "        if j >= 9:\n",
    "            break\n",
    "    if i >= 10:\n",
    "        break\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federate a dataset from torchtext, torchvision, tensorflow or huggingface\n",
    "\n",
    "There are a lot of datasets available on the main deep learning frameworks that make it easy to use their own framework. It's important to let the user to use this datasets, and we do so the user can use her favorites datasets and federate them.\n",
    "\n",
    "We show multiple examples on how to load and federate the datasets.\n",
    "\n",
    "For every framework we support, there are two possible ways to load a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flex.data import Dataset\n",
    "from flex.data import FedDataDistribution\n",
    "from flex.data import FedDatasetConfig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HugginFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load a dataset into a Dataset\n",
    "dataset_hf = load_dataset(\"ag_news\", split=\"train\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcd_hf = Dataset.from_huggingface_dataset(\n",
    "    dataset_hf, X_columns=[\"text\"], label_columns=[\"label\"]\n",
    ")\n",
    "\n",
    "# Create a config and federate the dataset\n",
    "config_hf = FedDatasetConfig(seed=0, n_nodes=2, replacement=False)\n",
    "\n",
    "\n",
    "flex_dataset_two_step_hf = FedDataDistribution.from_config(\n",
    "    centralized_data=fcd_hf, config=config_hf\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Flex dataset two steps a data sample from client_0: {flex_dataset_two_step_hf[0].X_data[0]}\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or another (shortcut):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Federate the dataset directly, only using a config.\n",
    "flex_dataset_hf = FedDataDistribution.from_config_with_huggingface_dataset(\n",
    "    dataset_hf, config_hf, [\"text\"], [\"label\"]\n",
    ")\n",
    "\n",
    "print(f\"Flex dataset a data sample from client_0: {flex_dataset_hf[0].X_data[0]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "ds_train, ds_test = tfds.load(\n",
    "    \"mnist\",\n",
    "    split=[\"train\", \"test\"],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    batch_size=-1,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcd_tf = Dataset.from_tfds_image_dataset(ds_train)\n",
    "\n",
    "config_tf = FedDatasetConfig(seed=0, n_nodes=2, replacement=False)\n",
    "\n",
    "\n",
    "# Federate the Dataset we just created\n",
    "flex_dataset_two_step_tf = FedDataDistribution.from_config(\n",
    "    centralized_data=fcd_tf, config=config_tf\n",
    ")\n",
    "\n",
    "sample = flex_dataset_two_step_tf[0].X_data[0]\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(sample, cmap=plt.get_cmap(\"gray\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Federate the dataset directly\n",
    "flex_dataset_tf = FedDataDistribution.from_config_with_tfds_image_dataset(\n",
    "    ds_train, config_tf\n",
    ")\n",
    "\n",
    "sample = flex_dataset_tf[0].X_data[0]\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(sample, cmap=plt.get_cmap(\"gray\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch torchvision dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "cifar10 = datasets.CIFAR10(root=\".\", train=True, download=True, transform=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcd_torch = Dataset.from_torchvision_dataset(cifar10)\n",
    "\n",
    "config_torch = FedDatasetConfig(seed=0, n_nodes=2, replacement=False)\n",
    "\n",
    "# Federate the Dataset we just created\n",
    "flex_dataset_two_step_torch = FedDataDistribution.from_config(\n",
    "    centralized_data=fcd_torch, config=config_torch\n",
    ")\n",
    "\n",
    "sample = flex_dataset_two_step_torch[0].X_data[0]\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(sample, cmap=plt.get_cmap(\"gray\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or another (shortcut):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Federate the dataset directly\n",
    "flex_dataset_torch = FedDataDistribution.from_config_with_torchvision_dataset(\n",
    "    cifar10, config_tf\n",
    ")\n",
    "\n",
    "sample = flex_dataset_torch[0].X_data[0]\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(sample, cmap=plt.get_cmap(\"gray\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch torchtext dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import AG_NEWS\n",
    "\n",
    "torch_dataset = AG_NEWS(split=\"train\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcd_torch = Dataset.from_torchtext_dataset(torch_dataset)\n",
    "\n",
    "# We will use the same configuration than in the HuggingFace example\n",
    "config_torch = FedDatasetConfig(seed=0, n_nodes=2, replacement=False)\n",
    "\n",
    "# Federate the Dataset we just created\n",
    "flex_dataset_two_step_torch = FedDataDistribution.from_config(\n",
    "    centralized_data=fcd_torch, config=config_torch\n",
    ")\n",
    "\n",
    "print(f\"Flex dataset two steps: {flex_dataset_two_step_torch[0].X_data[0]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or another (shortcut):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Federate the dataset directly\n",
    "flex_dataset_torch = FedDataDistribution.from_config_with_torchtext_dataset(\n",
    "    torch_dataset, config_torch\n",
    ")\n",
    "print(f\"Flex dataset direct: {flex_dataset_torch[0].X_data[0]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END\n",
    "Congratulations, now you know how to federate a dataset using the *FedDataDistribution* and the *FedDatasetConfig* classes, so you can setup multiple experimental settings that fit most your hipothesis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "4f7d2a9c5c2a9c2b510c9da0e3374aeab36a589c02bbbebadbda47bfb5610ebb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
