{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLEXible tutorial: Text classification using *Transformers*\n",
    "\n",
    "FLEXible is a library to federate models. We offer the tools to load and federate data or to load federated data, and the tools to create a federated environment. The user must define the model and the *communication primitives* to train the model in a federated environment. This primitives can be expressed in the following steps:\n",
    "- initialization: Initialize the model in the server.\n",
    "- deplot model: Deploy the model to the clients.\n",
    "- training: Define the train function.\n",
    "- collect the weights: Collect the weights of the clients params to aggregate them later.\n",
    "- aggregate the weights: Use an aggregation method to aggregte the collected weights.\n",
    "- deploy model: Deploy the model with the updated weights to the clients.\n",
    "- evaluate: Define the evaluate function.\n",
    "\n",
    "In this notebook, we show how to implement this primitives and how to use FLEXible in orther to federate a model using Huggingface's library *transformers*. In this way, we will train a model using multiple clients, but without sharing any data between clients. We will follow this [tutorial](https://huggingface.co/docs/transformers/training) from the Huggingface tutorials for text classification. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "\n",
    "from datasets.load import load_dataset\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "TRANSFORMER_MODEL = \"distilbert-base-uncased\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the IMBD dataset\n",
    "\n",
    "In the tutorial the dataset used is the Yilp Reviews, but in FLEXible we have some Pluggable Datasets, which we can directly adapt to the FLEXible data structure. In this case, we will use the IMDB dataset, as it is commonly used as an example for text classification, and it is the one we're using in the tutorials for text classification when using FLEXible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag_news_dataset = load_dataset('imdb', split=['train', 'test']) # Get the dataset from huggingface library"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now show the structure of the dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then select the train-test partition, so we can federate the train data between clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples, test_examples = ag_news_dataset[0], ag_news_dataset[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) From centralized data to federated data\n",
    "Usually we would have to encapsulate our centralized dataset as numpy arrays in a Dataset, to split it for every federated client. As we have so Pluggable Datasets for *huggingface*, *torch* and *tensorflow*, we can directly create a configuration within a ``FedDatasetConfig`` object. For this case we want to split it evenly between 2 clients, that is, an iid distribution.\n",
    "\n",
    "To apply our config to the dataset, we use ``FedDataDistribution.from_config_with_huggingface_dataset``, so we will federate the dataset as expected. A more complete description of the configuration options of ``FedDatasetConfig``to federate a dataset can be found in the documentation. Also, it is highly recommended to check if the desired dataset is supported in the ``PluggableDatasets``as it will be directly loaded to FLEXible as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flex.data import FedDatasetConfig, FedDataDistribution\n",
    "\n",
    "config = FedDatasetConfig(seed=0)\n",
    "config.n_clients = 2\n",
    "config.replacement = False # ensure that clients do not share any data\n",
    "config.client_names = ['client1', 'client2'] # Optional\n",
    "flex_dataset = FedDataDistribution.from_config_with_huggingface_dataset(data=train_examples, config=config, X_columns='text', label_columns='label')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Federating a model with FLEXible\n",
    "\n",
    "Once we've federated the dataset, we have to create the FlexPool. The FlexPool class simulates a real-time scenario for federated learning, so it is in charge of the communications across the actors. The class FlexPool will assign to each actor a role (client, aggregator, server), so they can communicate during the training phase.\n",
    "\n",
    "Please, check the notebook about the actors (TODO: Hacer notebook actores y sus relaciones) to know more about the actors and their relationships in FLEXible.\n",
    "\n",
    "To create a Pool of actors, we need to have a federated dataset, like we've just done, and the model to initialize in the server side, because the server will send the model to the clients so they can train the model. As we have the federated dataset (flex_dataset), we will now create the model.\n",
    "\n",
    "In this case, we will use a model from the tensorflow hub, so we dont have to worry about coding it. We also consider a federated setup commonly know as client server architecture, where a server orchestrates the training of federated clients in every round.\n",
    "\n",
    "In the following, we create a client server architecture and provide a function to initialize the server model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flex.model import FlexModel\n",
    "\n",
    "from flex.pool.decorators import init_server_model\n",
    "from flex.pool.decorators import deploy_server_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@init_server_model\n",
    "def define_model(*args):\n",
    "    flex_model = FlexModel()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL)\n",
    "    model =  AutoModelForSequenceClassification.from_pretrained(TRANSFORMER_MODEL, num_labels=2)\n",
    "    flex_model['model'] = model\n",
    "    flex_model['tokenizer'] = tokenizer\n",
    "    return flex_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flex.pool import FlexPool\n",
    "\n",
    "flex_pool = FlexPool.client_server_architecture(fed_dataset=flex_dataset, init_func=define_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients = flex_pool.clients\n",
    "server = flex_pool.servers\n",
    "aggregators = flex_pool.aggregators\n",
    "print(f\"Server node is indentified by {server.actor_ids}\")\n",
    "print(f\"Client nodes are identified by {clients.actor_ids}\")\n",
    "print(f\"Aggregator nodes are identified by {aggregators.actor_ids}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@deploy_server_model is a decorator designed to copy the model from the server to the clients at each federated learning round. The function that uses it, must have at least one argument, which is the FlexModel object that stores the model at the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "@deploy_server_model\n",
    "def copy_server_model_to_clients(server_flex_model: FlexModel):\n",
    "    flex_model = FlexModel()\n",
    "    for k, v in server_flex_model.items():\n",
    "        flex_model[k] = copy.deepcopy(v)\n",
    "    return flex_model\n",
    "\n",
    "server.map(copy_server_model_to_clients, clients)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suprisingly, there is no decorator for the training process as it can be imnplemented directly. As we are using PyTorch, we have to create the PyTorch dataset that will be fed into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom class\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "\n",
    "class IMDbDataset(TorchDataset):\n",
    "    def __init__(self, encodings, labels) -> None:\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are using the *transformers* library, we can use the **Trainer** class to automatize the train process. In case you prefer to create your own train function for the model, you hace to create your train loop function. Here we show both options, one with the *Trainer* class, and one with a classic train loop for PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Trainer or native PyTorch.\n",
    "\n",
    "from flex.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "\n",
    "def tokenize_function(texts, tokenizer):\n",
    "    return tokenizer(texts, padding=\"max_length\", truncation=True)\n",
    "\n",
    "def train_loop(model, train_dataloader, num_epochs, device, optimizer):\n",
    "    for _ in range(num_epochs):\n",
    "        for batch in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "def train_native_pt(client_flex_model: FlexModel, client_data: Dataset, tokenize_func, train_loop_func):\n",
    "    X_data = tokenize_func(client_data.X_data.tolist(), client_flex_model['tokenizer'])\n",
    "    imdb_dataset = IMDbDataset(X_data, client_data.y_data)\n",
    "    imdb_loader = DataLoader(imdb_dataset, shuffle=True, batch_size=16)\n",
    "    optimizer = AdamW(client_flex_model.model.parameters(), lr=5e-5)\n",
    "    num_epochs = 3\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    train_loop_func(client_flex_model['model'], imdb_loader, num_epochs, device, optimizer)\n",
    "\n",
    "clients.map(train_native_pt, tokenize_func=tokenize_function, train_loop_func=train_loop)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the model is trained, we have to collect the weights from the clients, so we can aggregate them. At FLEXible exists a primitive to collect those weights for a neural network. You can use this funcion, or you can create your own function. Also, in FLEXible exists a funcion to set the aggregated weihts for PyTorch models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flex.pool.primitives import collect_clients_weights_pt, set_aggregated_weights_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregators.map(collect_clients_weights_pt, clients)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the aggregation it is possible to implement your own aggregation function with the *aggregate_weights* decorator, or we can use the aggregators that are already implemented in FLEXible, such as FedAvg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flex.pool.aggregators import fed_avg\n",
    "\n",
    "aggregators.map(fed_avg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After aggregating the weights, we use the set_aggregated_weights for PyTorch, that set the aggregated weights to the server model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregators.map(set_aggregated_weights_pt, server)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model and setting the weights onto the server model, we can just evaluate it using the *evaluate_server_model* decorator. This decorator is created to test the server model using external data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flex.pool import evaluate_server_model\n",
    "import evaluate\n",
    "\n",
    "def eval_loop(model, eval_dataloader, device, metric):\n",
    "    model.eval()\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    print(f\"Results: {metric.compute()}\")\n",
    "\n",
    "@evaluate_server_model\n",
    "def evaluate_global_model(server_flex_model: FlexModel, test_data=None, tokenize_func=None, eval_func=None):\n",
    "    X_data = tokenize_func(test_data.X_data.tolist()[:100], server_flex_model['tokenizer']) # Using subset, for testing purposes.\n",
    "    imdb_dataset = IMDbDataset(X_data, test_data.y_data[:100]) # Using subset, for testing purposes.\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    imdb_dataloader = DataLoader(imdb_dataset, batch_size=1, shuffle=False)\n",
    "    eval_func(server_flex_model['model'], imdb_dataloader, device, metric)\n",
    "\n",
    "test_data = Dataset.from_huggingface_dataset(test_examples, X_columns='text', label_columns='label')\n",
    "server.map(evaluate_global_model, test_data=test_data, tokenize_func=tokenize_function, eval_func=eval_loop)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the federated learning experiment for a few rounds\n",
    "\n",
    "Now, we can summarize the steps provided above and run the federated experiment for multiple rounds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_n_rounds(n_rounds, clients_per_round=2):\n",
    "    pool = FlexPool.client_server_architecture(fed_dataset=flex_dataset, init_func=define_model)\n",
    "    for i in range(n_rounds):\n",
    "        print(f\"\\nRunning round: {i+1} of {n_rounds+1}\")\n",
    "        node_dropout = 1-(clients_per_round/len(pool.clients))\n",
    "        selected_clients_pool = pool.clients.filter(node_dropout=node_dropout)\n",
    "        selected_clients = selected_clients_pool.clients\n",
    "        print(f\"Selected clients for this round: {len(selected_clients)}\")\n",
    "        # Deploy the server model to the selected clients\n",
    "        pool.servers.map(copy_server_model_to_clients, selected_clients)\n",
    "        # Each selected client trains her model\n",
    "        selected_clients.map(train_native_pt, tokenize_func=tokenize_function, train_loop_func=train_loop)\n",
    "        # The aggregador collects weights from the selected clients and aggregates them\n",
    "        pool.aggregators.map(collect_clients_weights_pt, selected_clients)\n",
    "        # Apply FedAvg aggregator\n",
    "        pool.aggregators.map(fed_avg)\n",
    "        # The aggregator send its aggregated weights to the server\n",
    "        pool.aggregators.map(set_aggregated_weights_pt, pool.servers)\n",
    "        pool.servers.map(evaluate_global_model, test_data=test_data, tokenize_func=tokenize_function, eval_func=eval_loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_n_rounds(n_rounds=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END\n",
    "Congratulations, now you know how to train a model using FLEXible for multiples rounds using the *HuggingFace* ecosystem with PyTorch as Deep Learning framawork. Remember that it's important to first deploy/initialize the model on the clients, so you can run the rounds without problem!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flexible",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
