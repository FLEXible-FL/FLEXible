{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLEXible tutorial: Two types of updating weights using Pytorch\n",
    "\n",
    "FLEXible is a library to federate models. We offer the tools to load and federate data or to load federated data, and the tools to create a federated environment. The user can define the model and the *communications primitives* to train the model in a federated environment, but we already offer decorators so that an advaced user can implement its own federated workflow. In addition, we offer some primitive functions for Pytorch. Those primitives are basics functions, as we assume how the federated learning training will be. If you want to a more customizable training loop, please check the notebook \"Federated MNIST PT example with flexible decorators\", as we show there how to implement the primitive functions from scrach. \n",
    "\n",
    "In this notebook, we show the two types of updating the weights of the client's model in federated learning. These methods are:\n",
    "- Updating the weights from the local models for the global weights recieved.\n",
    "- Updating the local weights by adding the difference between the global weights recieved and the local weights from the last round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First load the data, in this case we've chosen the MNIST dataset. This dataset is directy plugged onto FLEXible, so we can use the load function to get it directly federated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[sultan]: md5 -q ./emnist-digits.mat;\n",
      "[sultan]: md5 -q ./emnist-digits.mat;\n"
     ]
    }
   ],
   "source": [
    "from flex.datasets import load\n",
    "from torchvision import transforms\n",
    "\n",
    "flex_dataset, test_data = load(\"federated_emnist\", return_test=True, split=\"digits\")\n",
    "\n",
    "mnist_transforms = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5,), (0.5,))\n",
    "        ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`@init_server_model` is a decorator designed to perform the initialization of the server model in a client-server architecture. It has no requirements for specific arguments in the function that uses it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes in the pool 3580: 1 server plus 3579 clients. The server is also an aggregator\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from flex.pool import init_server_model\n",
    "from flex.pool import FlexPool\n",
    "from flex.model import FlexModel\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "@init_server_model\n",
    "def build_server_model():\n",
    "    server_flex_model = FlexModel()\n",
    "\n",
    "    server_flex_model[\"model\"] = SimpleNet().to(device) # Ensure all models are on the same device\n",
    "    # Required to store this for later stages of the FL training process\n",
    "    server_flex_model[\"criterion\"] = torch.nn.CrossEntropyLoss()\n",
    "    server_flex_model[\"optimizer_func\"] = torch.optim.Adam\n",
    "    server_flex_model[\"optimizer_kwargs\"] = {}\n",
    "\n",
    "    return server_flex_model\n",
    "\n",
    "flex_pool = FlexPool.client_server_architecture(flex_dataset, init_func=build_server_model)\n",
    "\n",
    "clients = flex_pool.clients\n",
    "servers = flex_pool.servers\n",
    "aggregators = flex_pool.aggregators\n",
    "\n",
    "print(f\"Number of nodes in the pool {len(flex_pool)}: {len(servers)} server plus {len(clients)} clients. The server is also an aggregator\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also implement the possibility of select a subsample of the clients in the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server node is indentified by key \"server\"\n",
      "Selected 2 client nodes of a total of 3579\n"
     ]
    }
   ],
   "source": [
    "#Select clients\n",
    "clients_per_round=2\n",
    "selected_clients_pool = clients.select(clients_per_round)\n",
    "selected_clients = selected_clients_pool.clients\n",
    "\n",
    "print(f\"Server node is indentified by key \\\"{servers.actor_ids[0]}\\\"\")\n",
    "print(f\"Selected {len(selected_clients.actor_ids)} client nodes of a total of {len(clients.actor_ids)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`@deploy_server_model` is a decorator designed to copy the model from the server to the clients at each federated learning round. The function that uses it, must have at least one argument, which is the FlexModel object that stores the model at the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flex.pool.primitives_pt import deploy_server_model_pt\n",
    "\n",
    "servers.map(deploy_server_model_pt, selected_clients)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two ways of updateing the weights of the client's model.\n",
    "\n",
    "Suprisingly, there is no decorator for the training process as it can be imnplemented directly. In this notebook we present primitives for both ways of updating the weights when using Pytorch, so the user can use them without needing to implement a new one, but if a different way of updating the weights it's needed, it can be created using the decorators for collecting and sendings weights. For more info check the notebook **Federated MNIST PT example with flexible decorators**, where we show how to create your own primitives.\n",
    "\n",
    "### Updating the weights from the local models for the global weights recieved.\n",
    "\n",
    "First we show how to update the weights by just changing the weights by the new ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flex.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(client_flex_model: FlexModel, client_data: Dataset):\n",
    "    train_dataset = client_data.to_torchvision_dataset(transform=mnist_transforms)\n",
    "    client_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "    model = client_flex_model[\"model\"]\n",
    "    # client_flex_model['previous_model'] = deepcopy(model) # Required to use `collect_client_diff_weights_pt` primitive\n",
    "    optimizer = client_flex_model['optimizer_func'](model.parameters(), **client_flex_model[\"optimizer_kwargs\"])\n",
    "    model = model.train()\n",
    "    model = model.to(device)\n",
    "    criterion = client_flex_model[\"criterion\"]\n",
    "    epochs = 5\n",
    "    for _ in tqdm(range(epochs)):\n",
    "        for imgs, labels in client_dataloader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(imgs)\n",
    "            loss = criterion(pred, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 25.72it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 51.96it/s]\n"
     ]
    }
   ],
   "source": [
    "selected_clients.map(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primitives for updating the weights by just changing one weights for other are `collect_clients_weights_pt` and `set_aggregated_weights_pt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flex.pool import collect_clients_weights_pt, set_aggregated_weights_pt\n",
    "\n",
    "aggregators.map(collect_clients_weights_pt, selected_clients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fed_avg` implements the aggregator Fedeverated Average, which computes the mean of the collected weights in previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flex.pool import fed_avg\n",
    "\n",
    "aggregators.map(fed_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the weights are aggregated, we can send them to the clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregators.map(set_aggregated_weights_pt, servers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`@evaluate_server_model` is a decorator used to test the server model. The function that uses it must have at least one argument, the FlexModel at the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flex.pool import evaluate_server_model\n",
    "\n",
    "@evaluate_server_model\n",
    "def evaluate_global_model(server_flex_model: FlexModel, test_data=None):\n",
    "    model = server_flex_model[\"model\"]\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    total_count = 0\n",
    "    model = model.to(device)\n",
    "    criterion=server_flex_model['criterion']\n",
    "    # get test data as a torchvision object\n",
    "    test_dataset = test_data.to_torchvision_dataset(transform=mnist_transforms)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=256, shuffle=True, pin_memory=False)\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_dataloader:\n",
    "            total_count += target.size(0)\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            losses.append(criterion(output, target).item())\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            test_acc += pred.eq(target.data.view_as(pred)).long().cpu().sum().item()\n",
    "\n",
    "    test_loss = sum(losses) / len(losses)\n",
    "    test_acc /= total_count\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2.1045245008104168, 0.136775)\n"
     ]
    }
   ],
   "source": [
    "metrics = servers.map(evaluate_global_model, test_data=test_data)\n",
    "print(metrics[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the weights by calculating the difference between the global and the local model.\n",
    "\n",
    "Lastly, we show the process on how to update the weights by calculating the diference between the global model and the local model of the clients. To do so, we need to add a new key to the client's FlexModel, *previous_model*, to copy the global model recieved by the last round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flex.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(client_flex_model: FlexModel, client_data: Dataset):\n",
    "    train_dataset = client_data.to_torchvision_dataset(transform=mnist_transforms)\n",
    "    client_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "    model = client_flex_model[\"model\"].to(device)\n",
    "    client_flex_model['previous_model'] = deepcopy(model) # Required to use `collect_client_diff_weights_pt` primitive\n",
    "    optimizer = client_flex_model['optimizer_func'](model.parameters(), **client_flex_model[\"optimizer_kwargs\"])\n",
    "    model = model.train()\n",
    "    criterion = client_flex_model[\"criterion\"]\n",
    "    epochs = 5\n",
    "    for _ in tqdm(range(epochs)):\n",
    "        for imgs, labels in client_dataloader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(imgs)\n",
    "            loss = criterion(pred, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 206.84it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 119.28it/s]\n"
     ]
    }
   ],
   "source": [
    "selected_clients.map(train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`collect_client_diff_weights_pt` as it name says, it collects weights from a set of clients. Particularly, it collects the difference between the model before and after training, that is, what the model has learnt in its local training step. Also note that the weights of the model before training are assume to be stored using `previous_model` as key in the FlexModel of a client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flex.pool import collect_client_diff_weights_pt\n",
    "\n",
    "aggregators.map(collect_client_diff_weights_pt, selected_clients)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also use the `fed_avg` aggregator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flex.pool import fed_avg\n",
    "\n",
    "aggregators.map(fed_avg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`set_aggregated_diff_weights_pt` adds the aggregated weights to the weights of the server, it assumes that the aggregated weights have been collected using a similar logic to `collect_client_diff_weights_pt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flex.pool import set_aggregated_diff_weights_pt\n",
    "\n",
    "aggregators.map(set_aggregated_diff_weights_pt, servers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the server model, we use the same function created in the last section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.590213431674204, 0.5245)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "metrics = servers.map(evaluate_global_model, test_data=test_data)\n",
    "print(metrics[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the federated learning experiment for a few rounds\n",
    "\n",
    "Now, we can summarize the steps provided above and run the federated experiment for multiple rounds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliar function to clear unused gpu mem in clients\n",
    "def clean_up_models(client_model: FlexModel, _):\n",
    "    import gc\n",
    "    del client_model[\"model\"]\n",
    "    del client_model[\"previous_model\"]\n",
    "    gc.collect()\n",
    "\n",
    "def train_n_rounds(n_rounds, clients_per_round=20):  \n",
    "    pool = FlexPool.client_server_architecture(fed_dataset=flex_dataset, init_func=build_server_model)\n",
    "    for i in range(n_rounds):\n",
    "        print(f\"\\nRunning round: {i+1} of {n_rounds}\")\n",
    "        selected_clients_pool = pool.clients.select(clients_per_round)\n",
    "        selected_clients = selected_clients_pool.clients\n",
    "        print(f\"Selected clients for this round: {len(selected_clients)}\")\n",
    "        # Deploy the server model to the selected clients\n",
    "        pool.servers.map(deploy_server_model_pt, selected_clients)\n",
    "        # Each selected client trains her model\n",
    "        selected_clients.map(train)\n",
    "        # The aggregador collects weights from the selected clients and aggregates them\n",
    "        pool.aggregators.map(collect_client_diff_weights_pt, selected_clients)\n",
    "        pool.aggregators.map(fed_avg)\n",
    "        # The aggregator send its aggregated weights to the server\n",
    "        pool.aggregators.map(set_aggregated_diff_weights_pt, pool.servers)\n",
    "        # Optional: evaluate the server model\n",
    "        metrics = pool.servers.map(evaluate_global_model, test_data=test_data)\n",
    "        # Optional: clean-up unused memory\n",
    "        selected_clients.map(clean_up_models)\n",
    "        loss, acc = metrics[0]\n",
    "        print(f\"Server: Test acc: {acc:.4f}, test loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running round: 1 of 20\n",
      "Selected clients for this round: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 70.16it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 100.72it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 69.83it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 70.23it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 69.60it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 73.30it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 110.71it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 108.74it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 74.80it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 57.01it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, mps:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/teddy/Documents/FLEX-framework/notebooks/Two ways of updating weights with FLEXible.ipynb Celda 36\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bworkstation2012.duckdns.org/Users/teddy/Documents/FLEX-framework/notebooks/Two%20ways%20of%20updating%20weights%20with%20FLEXible.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m train_n_rounds(\u001b[39m20\u001b[39;49m, clients_per_round\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "\u001b[1;32m/Users/teddy/Documents/FLEX-framework/notebooks/Two ways of updating weights with FLEXible.ipynb Celda 36\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bworkstation2012.duckdns.org/Users/teddy/Documents/FLEX-framework/notebooks/Two%20ways%20of%20updating%20weights%20with%20FLEXible.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m pool\u001b[39m.\u001b[39maggregators\u001b[39m.\u001b[39mmap(fed_avg)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bworkstation2012.duckdns.org/Users/teddy/Documents/FLEX-framework/notebooks/Two%20ways%20of%20updating%20weights%20with%20FLEXible.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# The aggregator send its aggregated weights to the server\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bworkstation2012.duckdns.org/Users/teddy/Documents/FLEX-framework/notebooks/Two%20ways%20of%20updating%20weights%20with%20FLEXible.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m pool\u001b[39m.\u001b[39;49maggregators\u001b[39m.\u001b[39;49mmap(set_aggregated_diff_weights_pt, pool\u001b[39m.\u001b[39;49mservers)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bworkstation2012.duckdns.org/Users/teddy/Documents/FLEX-framework/notebooks/Two%20ways%20of%20updating%20weights%20with%20FLEXible.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# Optional: evaluate the server model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bworkstation2012.duckdns.org/Users/teddy/Documents/FLEX-framework/notebooks/Two%20ways%20of%20updating%20weights%20with%20FLEXible.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m metrics \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39mservers\u001b[39m.\u001b[39mmap(evaluate_global_model, test_data\u001b[39m=\u001b[39mtest_data)\n",
      "File \u001b[0;32m~/Documents/FLEX-framework/flex/pool/pool.py:120\u001b[0m, in \u001b[0;36mFlexPool.map\u001b[0;34m(self, func, dst_pool, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m     res \u001b[39m=\u001b[39m [\n\u001b[1;32m    116\u001b[0m         func(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_models\u001b[39m.\u001b[39mget(i), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data\u001b[39m.\u001b[39mget(i), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    117\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_actors\n\u001b[1;32m    118\u001b[0m     ]\n\u001b[1;32m    119\u001b[0m \u001b[39melif\u001b[39;00m FlexPool\u001b[39m.\u001b[39mcheck_compatibility(\u001b[39mself\u001b[39m, dst_pool):\n\u001b[0;32m--> 120\u001b[0m     res \u001b[39m=\u001b[39m [\n\u001b[1;32m    121\u001b[0m         func(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_models\u001b[39m.\u001b[39mget(i), dst_pool\u001b[39m.\u001b[39m_models, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    122\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_actors\n\u001b[1;32m    123\u001b[0m     ]\n\u001b[1;32m    124\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    126\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSource and destination pools are not allowed to comunicate, ensure that their actors can communicate.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    127\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/FLEX-framework/flex/pool/pool.py:121\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    115\u001b[0m     res \u001b[39m=\u001b[39m [\n\u001b[1;32m    116\u001b[0m         func(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_models\u001b[39m.\u001b[39mget(i), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data\u001b[39m.\u001b[39mget(i), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    117\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_actors\n\u001b[1;32m    118\u001b[0m     ]\n\u001b[1;32m    119\u001b[0m \u001b[39melif\u001b[39;00m FlexPool\u001b[39m.\u001b[39mcheck_compatibility(\u001b[39mself\u001b[39m, dst_pool):\n\u001b[1;32m    120\u001b[0m     res \u001b[39m=\u001b[39m [\n\u001b[0;32m--> 121\u001b[0m         func(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_models\u001b[39m.\u001b[39;49mget(i), dst_pool\u001b[39m.\u001b[39;49m_models, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    122\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_actors\n\u001b[1;32m    123\u001b[0m     ]\n\u001b[1;32m    124\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    126\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSource and destination pools are not allowed to comunicate, ensure that their actors can communicate.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    127\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/FLEX-framework/flex/pool/decorators.py:92\u001b[0m, in \u001b[0;36mset_aggregated_weights.<locals>._deploy_aggregated_weights_\u001b[0;34m(aggregator_flex_model, servers_flex_models, *args, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     85\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_deploy_aggregated_weights_\u001b[39m(\n\u001b[1;32m     86\u001b[0m     aggregator_flex_model: FlexModel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m     90\u001b[0m ):\n\u001b[1;32m     91\u001b[0m     \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m servers_flex_models:\n\u001b[0;32m---> 92\u001b[0m         func(\n\u001b[1;32m     93\u001b[0m             servers_flex_models[k],\n\u001b[1;32m     94\u001b[0m             aggregator_flex_model[\u001b[39m\"\u001b[39;49m\u001b[39maggregated_weights\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     95\u001b[0m             \u001b[39m*\u001b[39;49margs,\n\u001b[1;32m     96\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m     97\u001b[0m         )\n",
      "File \u001b[0;32m~/Documents/FLEX-framework/flex/pool/primitives_pt.py:225\u001b[0m, in \u001b[0;36mset_aggregated_diff_weights_pt\u001b[0;34m(server_flex_model, aggregated_diff_weights, *args, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    224\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(new) \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:  \u001b[39m# Do not copy empty layers\u001b[39;00m\n\u001b[0;32m--> 225\u001b[0m         weight_dict[layer_key]\u001b[39m.\u001b[39;49madd_(new)\n\u001b[1;32m    226\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:  \u001b[39m# new has no len property\u001b[39;00m\n\u001b[1;32m    227\u001b[0m     weight_dict[layer_key]\u001b[39m.\u001b[39madd_(new)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, mps:0 and cpu!"
     ]
    }
   ],
   "source": [
    "train_n_rounds(20, clients_per_round=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('snowflakes')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e566fdd1bb8982d94cfadd1b83b7d24c79e08c8cc9caeab62a639b678958baae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
