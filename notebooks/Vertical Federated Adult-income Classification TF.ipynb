{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vertical Federated Learning with the Adult-income dataset\n",
    "\n",
    "In this notebook we show how to use FLEXible to simulate a Vertical Federated Learning (VFL) scenario with a neural network using Tensorflow. We implement the VFL process described in paper: [Vertical Federated Learning: Challenges, Methodologies and Experiments](https://arxiv.org/abs/2202.04309)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we download the raw dataset using the `ucimlrepo` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flex.model import FlexModel\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# fetch dataset\n",
    "adult = fetch_ucirepo(id=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We preprocess it, making the targets integers and removing rows with NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# data (as pandas dataframes)\n",
    "x_cols = adult.data.features.columns.tolist()\n",
    "y_cols = adult.data.targets.columns.tolist()\n",
    "adult = pd.concat([adult.data.features, adult.data.targets], axis=1)\n",
    "adult = adult.dropna()\n",
    "x_data = adult[x_cols]\n",
    "y_data = adult[y_cols]\n",
    "# Transform string labels \">50K\", \"<=50K\" to integer labels 1, 0\n",
    "y_data = y_data[\"income\"].apply(lambda label: \">\" in label).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a `Dataset` object using the preprocessed dataset and a `FedDatasetConfig` object showing how we want to simulate the vertical split: \n",
    "\n",
    "There are three nodes, one `host` and two `guest`, only the former keeps the labels and each one has 4 features. As we are performing a vertical split, we need to provide weights equal to one, `replacement=False` and `shuffle=False`, to ensure that each node gets the same data points but different features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flex.data import Dataset, FedDatasetConfig, FedDataDistribution\n",
    "\n",
    "dataset = Dataset.from_array(x_data.to_numpy(), y_data.to_numpy())\n",
    "\n",
    "config = FedDatasetConfig(\n",
    "    seed=0,\n",
    "    n_nodes=3,\n",
    "    node_ids=[\"host\", \"guest_1\", \"guest_2\"],\n",
    "    shuffle=False,\n",
    "    replacement=True,\n",
    "    keep_labels=[True, False, False],\n",
    "    weights=[\n",
    "        1.0,\n",
    "        1.0,\n",
    "        1.0,\n",
    "    ],  # Ensure that each node gets the entire dataset, not a subset\n",
    "    features_per_node=[\n",
    "        [0, 1, 2, 3],  # ['age', 'workclass', 'fnlwgt', 'education'] for \"host\" node\n",
    "        [\n",
    "            4,\n",
    "            5,\n",
    "            6,\n",
    "            7,\n",
    "        ],  # ['education-num', 'marital-status', 'occupation', 'relationship'] for \"guest_1\" node\n",
    "        [\n",
    "            8,\n",
    "            9,\n",
    "            10,\n",
    "            11,\n",
    "            12,\n",
    "            13,\n",
    "        ],  # ['race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country'] for \"guest_2\" node\n",
    "    ],\n",
    ")\n",
    "\n",
    "fed_dataset = FedDataDistribution.from_config(dataset, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have split the data among nodes, we one hot encode each local dataset. Beware, only the \"host\" node has labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def preprocess_x_data(local_dataset: Dataset):\n",
    "    ohe_encoder = preprocessing.OneHotEncoder(\n",
    "        sparse_output=False, handle_unknown=\"ignore\"\n",
    "    )\n",
    "    x_data = local_dataset.X_data.to_numpy()\n",
    "    y_data = local_dataset.y_data\n",
    "    encoded_x_data = ohe_encoder.fit_transform(x_data)\n",
    "    return Dataset.from_array(encoded_x_data, y_data)\n",
    "\n",
    "\n",
    "# One hot encode each dataset individually\n",
    "fed_dataset = fed_dataset.apply(preprocess_x_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once data is federated, we assign a `FlexRole` to each `node_id`, to create a `FlexPool` which simulates the Vertical Federated Learning flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flex.actors import FlexRole, FlexActors\n",
    "\n",
    "actors = FlexActors()\n",
    "actors[\"host\"] = FlexRole.server_aggregator_client\n",
    "actors[\"guest_1\"] = FlexRole.client\n",
    "actors[\"guest_2\"] = FlexRole.client\n",
    "\n",
    "from flex.pool import FlexPool\n",
    "\n",
    "pool = FlexPool(fed_dataset, actors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VFL model in this simulation is a simple fully-connected network with three layers of 48, 96 and 196 hidden units. The bottom model has 48 hidden units and its output size is 32. The top model has 196 hidden units, its input size is 32*3=96 and it outputs the final predictions of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Hide GPU from visible devices, thus forcing CPU usage\n",
    "tf.config.set_visible_devices([], \"GPU\")\n",
    "\n",
    "\n",
    "def get_MLP_model(input_dim, hidden_units, out_dim):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.Input(shape=(input_dim,)))\n",
    "    model.add(\n",
    "        tf.keras.layers.Dense(\n",
    "            hidden_units,\n",
    "            kernel_regularizer=tf.keras.regularizers.L2(0.01),\n",
    "            activation=None,\n",
    "        )\n",
    "    )\n",
    "    model.add(\n",
    "        tf.keras.layers.Dense(\n",
    "            out_dim, kernel_regularizer=tf.keras.regularizers.L2(0.01), activation=None\n",
    "        )\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each node owns the bottom model, which has 48 hidden units and its output dimension is 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bottom tape is shared among all bottom model owners\n",
    "bottom_tape = tf.GradientTape(persistent=True)\n",
    "\n",
    "\n",
    "def get_bottom_tape():\n",
    "    return bottom_tape\n",
    "\n",
    "\n",
    "def initialize_bottom_model(local_model: FlexModel, local_dataset: Dataset):\n",
    "    input_dim = local_dataset.X_data.to_numpy().shape[-1]\n",
    "    hidden_units = 48\n",
    "    out_dim = 32\n",
    "    local_model[\"bottom_model\"] = get_MLP_model(input_dim, hidden_units, out_dim)\n",
    "    local_model[\"bottom_optimizer\"] = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
    "    local_model[\"bottom_tape\"] = get_bottom_tape()\n",
    "\n",
    "\n",
    "pool.map(initialize_bottom_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The host node also has the top part of the model, which receives as input the outputs of the bottom model, this its input dimension is 32*3=96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_top_model(local_model: FlexModel, local_dataset: Dataset):\n",
    "    input_dim = 32 * 3\n",
    "    hidden_units = 196\n",
    "    out_dim = 1\n",
    "    local_model[\"loss_f\"] = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    local_model[\"top_model\"] = get_MLP_model(input_dim, hidden_units, out_dim)\n",
    "    local_model[\"top_optimizer\"] = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
    "    # Top gradient tape is not shared as it is only owned by one node\n",
    "    local_model[\"top_tape\"] = tf.GradientTape(persistent=False)\n",
    "\n",
    "\n",
    "# The host has both the bottom and the top model\n",
    "host_pool = pool.select(lambda node_id, role: node_id == \"host\")\n",
    "host_pool.map(initialize_top_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions implement the logic of a FL round:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_batch_sampler(local_model: FlexModel, local_dataset: Dataset, seed=0):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    common_random_state = 1001\n",
    "    test_size = 0.2\n",
    "    batch_size = 500\n",
    "    # Perform train-test split here, in order to generate\n",
    "    if local_model.actor_id == \"host\":\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            local_dataset.X_data,\n",
    "            local_dataset.y_data,\n",
    "            test_size=test_size,\n",
    "            random_state=common_random_state,\n",
    "        )\n",
    "\n",
    "        def train_generator():\n",
    "            yield from zip(X_train, y_train)\n",
    "\n",
    "        def test_generator():\n",
    "            yield from zip(X_test, y_test)\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_generator(\n",
    "            train_generator,\n",
    "            output_types=(tf.float32, tf.float32),\n",
    "            output_shapes=(X_train[0].shape, ()),\n",
    "        )\n",
    "        test_dataset = tf.data.Dataset.from_generator(\n",
    "            test_generator,\n",
    "            output_types=(tf.float32, tf.float32),\n",
    "            output_shapes=(X_test[0].shape, ()),\n",
    "        )\n",
    "    else:\n",
    "        X_train, X_test = train_test_split(\n",
    "            local_dataset.X_data,\n",
    "            test_size=test_size,\n",
    "            random_state=common_random_state,\n",
    "        )\n",
    "\n",
    "        def train_generator():\n",
    "            yield from X_train\n",
    "\n",
    "        def test_generator():\n",
    "            yield from X_test\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_generator(\n",
    "            train_generator, output_types=(tf.float32), output_shapes=(X_train[0].shape)\n",
    "        )\n",
    "        test_dataset = tf.data.Dataset.from_generator(\n",
    "            test_generator, output_types=(tf.float32), output_shapes=(X_test[0].shape)\n",
    "        )\n",
    "\n",
    "    train_dataset = train_dataset.shuffle(batch_size, seed=seed).batch(batch_size)\n",
    "    test_dataset = test_dataset.batch(len(X_test))\n",
    "    local_model[\"train_batch_sampler\"] = iter(train_dataset)\n",
    "    local_model[\"test_batch_sampler\"] = iter(test_dataset)\n",
    "\n",
    "\n",
    "def generate_bottom_outputs(local_model: FlexModel, local_dataset: Dataset):\n",
    "    bottom_model = local_model[\"bottom_model\"]\n",
    "    try:\n",
    "        data = next(local_model[\"train_batch_sampler\"])\n",
    "        if local_model.actor_id == \"host\":\n",
    "            batch_data, local_model[\"y_batch\"] = data\n",
    "        else:\n",
    "            batch_data = data\n",
    "        with local_model[\"bottom_tape\"]:\n",
    "            local_model[\"bottom_output\"] = bottom_model(batch_data)\n",
    "        return True\n",
    "    except StopIteration:\n",
    "        return False\n",
    "\n",
    "\n",
    "def forward_bottom_outputs(host_model: FlexModel, models: dict):\n",
    "    top_model = host_model[\"top_model\"]\n",
    "    with host_model[\"top_tape\"], host_model[\"bottom_tape\"]:\n",
    "        bottom_outputs = []\n",
    "        for k in models:\n",
    "            bottom_outputs.append(models[k][\"bottom_output\"])\n",
    "        bottom_outputs = tf.concat(bottom_outputs, axis=1)\n",
    "        top_outputs = top_model(bottom_outputs)\n",
    "        logits = tf.nn.sigmoid(top_outputs)\n",
    "        host_model[\"computed_loss\"] = host_model[\"loss_f\"](\n",
    "            y_pred=logits, y_true=host_model[\"y_batch\"]\n",
    "        )\n",
    "\n",
    "\n",
    "def send_computed_loss(host_model: FlexModel, guest_models: dict):\n",
    "    for guest in guest_models:\n",
    "        guest_models[guest][\"computed_loss\"] = host_model[\"computed_loss\"]\n",
    "\n",
    "\n",
    "def backward_pass_top(local_model: FlexModel, local_dataset: Dataset):\n",
    "    top_grad = local_model[\"top_tape\"].gradient(\n",
    "        local_model[\"computed_loss\"], local_model[\"top_model\"].variables\n",
    "    )\n",
    "\n",
    "    local_model[\"top_optimizer\"].apply_gradients(\n",
    "        grads_and_vars=zip(top_grad, local_model[\"top_model\"].variables)\n",
    "    )\n",
    "\n",
    "\n",
    "def backward_pass_bottom(local_model: FlexModel, local_dataset: Dataset):\n",
    "    bottom_grad = local_model[\"bottom_tape\"].gradient(\n",
    "        local_model[\"computed_loss\"], local_model[\"bottom_model\"].variables\n",
    "    )\n",
    "\n",
    "    local_model[\"bottom_optimizer\"].apply_gradients(\n",
    "        grads_and_vars=zip(bottom_grad, local_model[\"bottom_model\"].variables)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above-defined functions an FL round is as follows:\n",
    "\n",
    "- First, all nodes agree to iterate their datasets in such a way that their features align using `setup_batch_sampler`.\n",
    "- Then, while possible each node generates the bottom output of its data using `generate_bottom_outputs`.\n",
    "- The bottom outputs are gathered by the host and feeded to the top model `forward_bottom_outputs`, additionally the loss is computed.\n",
    "- Send the computed loss to the guest nodes, so that every node can compute its backward pass using the computed loss in the top model.\n",
    "- Now, the backward pass begins, first the top model is updated using `backward_pass_top`, then the bottom models are updated using `backward_pass_bottom`.\n",
    "- This process is repeated until the nodes have iterated their full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guest_pool = pool.select(lambda node_id, role: node_id != \"host\")\n",
    "\n",
    "pool.map(setup_batch_sampler)\n",
    "\n",
    "while all(pool.map(generate_bottom_outputs)):\n",
    "    host_pool.map(forward_bottom_outputs, pool)\n",
    "    host_pool.map(send_computed_loss, guest_pool)\n",
    "    host_pool.map(backward_pass_top)\n",
    "    pool.map(backward_pass_bottom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still, we are missing the code to evaluate the performance of the whole model, which is as follows:\n",
    "\n",
    "1. Generate the bottom outputs of the test set\n",
    "1. The host gathers the bottom outputs, feeds them to the top model and computes the scores. Note that, the host node is the only node with labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_bottom_outputs(local_model: FlexModel, local_dataset: Dataset):\n",
    "    bottom_model = local_model[\"bottom_model\"]\n",
    "    data = next(local_model[\"test_batch_sampler\"])\n",
    "    if local_model.actor_id == \"host\":\n",
    "        batch_data, local_model[\"test_y\"] = data\n",
    "    else:\n",
    "        batch_data = data\n",
    "    local_model[\"test_bottom_output\"] = bottom_model(batch_data)\n",
    "\n",
    "\n",
    "def evalute_test_bottom_outputs(host_model: FlexModel, models):\n",
    "    from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "    bottom_outputs = []\n",
    "    for k in models:\n",
    "        bottom_outputs.append(models[k][\"test_bottom_output\"])\n",
    "    bottom_outputs = tf.concat(bottom_outputs, axis=1)\n",
    "    top_outputs = host_model[\"top_model\"](bottom_outputs)\n",
    "    logits = tf.nn.sigmoid(top_outputs)\n",
    "    logits = tf.squeeze(logits, axis=1)\n",
    "    preds = tf.cast(logits >= 0.5, tf.int32)\n",
    "    acc = accuracy_score(host_model[\"test_y\"], preds)\n",
    "    auc = roc_auc_score(host_model[\"test_y\"], logits)\n",
    "    print(f\"test auc: {auc:.4f}, test acc: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consequentially, a full VFL simulation, where every 5 rounds the whole model is evaluated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "fl_rounds = 100\n",
    "\n",
    "\n",
    "for i in range(fl_rounds):\n",
    "    globa_seed = randint(-1e10, 1e10)\n",
    "    pool.map(setup_batch_sampler, seed=globa_seed)\n",
    "    while all(pool.map(generate_bottom_outputs)):  # Train\n",
    "        host_pool.map(forward_bottom_outputs, pool)\n",
    "        host_pool.map(send_computed_loss, guest_pool)\n",
    "        host_pool.map(backward_pass_top)\n",
    "        pool.map(backward_pass_bottom)\n",
    "    if (i + 1) % 1 == 0:  # Evaluate\n",
    "        print(f\"FL Round {i+1}\", end=\" \")\n",
    "        pool.map(generate_test_bottom_outputs)\n",
    "        host_pool.map(evalute_test_bottom_outputs, pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
