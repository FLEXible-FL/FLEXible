{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributing an existing FLEXible experiment\n",
    "\n",
    "FLEXible is a framework primarily designed for simulating federated learning (FL) experiments locally. However, it also provides a convenient method for reusing existing code and deploying it in a genuine federated environment. In this notebook, we demonstrate how to create a distributed FLEXible environment by reusing code from our local experiments. Additionally, we highlight the key differences and important considerations that need attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The local experiment\n",
    "\n",
    "In the following experiment, we will train a Multi-Layer Perceptron (MLP) using the MNIST dataset. This example is explained in greater detail in the `Federated MNIST PT example with flexible decorators`. We recommend reviewing that example for a more comprehensive understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "from flex.data import Dataset\n",
    "from flex.datasets import load\n",
    "from flex.pool import init_server_model, deploy_server_model, collect_clients_weights, set_aggregated_weights, aggregate_weights\n",
    "from flex.pool import FlexPool\n",
    "from flex.model import FlexModel\n",
    "\n",
    "import tensorly as tl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "flex_dataset, test_data = load(\"federated_emnist\", return_test=True, split=\"digits\")\n",
    "\n",
    "# Assign test data to server_id\n",
    "server_id = \"server\"\n",
    "flex_dataset[server_id] = test_data\n",
    "\n",
    "\n",
    "mnist_transforms = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    ")\n",
    "\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "@init_server_model\n",
    "def build_server_model():\n",
    "    server_flex_model = FlexModel()\n",
    "\n",
    "    server_flex_model[\"model\"] = SimpleNet()\n",
    "    # Required to store this for later stages of the FL training process\n",
    "    server_flex_model[\"criterion\"] = torch.nn.CrossEntropyLoss()\n",
    "    server_flex_model[\"optimizer_func\"] = torch.optim.Adam\n",
    "    server_flex_model[\"optimizer_kwargs\"] = {}\n",
    "    return server_flex_model\n",
    "\n",
    "\n",
    "flex_pool = FlexPool.client_server_pool(\n",
    "    flex_dataset, server_id=server_id, init_func=build_server_model\n",
    ")\n",
    "\n",
    "\n",
    "clients = flex_pool.clients\n",
    "servers = flex_pool.servers\n",
    "aggregators = flex_pool.aggregators\n",
    "\n",
    "\n",
    "# Select clients\n",
    "clients_per_round = 20\n",
    "selected_clients_pool = clients.select(clients_per_round)\n",
    "selected_clients = selected_clients_pool.clients\n",
    "\n",
    "\n",
    "@deploy_server_model\n",
    "def copy_server_model_to_clients(server_flex_model: FlexModel):\n",
    "    return copy.deepcopy(server_flex_model)\n",
    "\n",
    "\n",
    "servers.map(copy_server_model_to_clients, selected_clients)\n",
    "\n",
    "\n",
    "def train(client_flex_model: FlexModel, client_data: Dataset):\n",
    "    train_dataset = client_data.to_torchvision_dataset(transform=mnist_transforms)\n",
    "    client_dataloader = DataLoader(train_dataset, batch_size=20)\n",
    "    model = client_flex_model[\"model\"]\n",
    "    optimizer = client_flex_model[\"optimizer_func\"](\n",
    "        model.parameters(), **client_flex_model[\"optimizer_kwargs\"]\n",
    "    )\n",
    "    model = model.train()\n",
    "    model = model.to(device)\n",
    "    criterion = client_flex_model[\"criterion\"]\n",
    "    for _ in range(5):\n",
    "        for imgs, labels in client_dataloader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(imgs)\n",
    "            loss = criterion(pred, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "selected_clients.map(train)\n",
    "\n",
    "\n",
    "@collect_clients_weights\n",
    "def get_clients_weights(client_flex_model: FlexModel):\n",
    "    weight_dict = client_flex_model[\"model\"].state_dict()\n",
    "    return [weight_dict[name] for name in weight_dict]\n",
    "\n",
    "\n",
    "aggregators.map(get_clients_weights, selected_clients)\n",
    "\n",
    "\n",
    "tl.set_backend(\"pytorch\")\n",
    "\n",
    "\n",
    "@aggregate_weights\n",
    "def aggregate_with_fedavg(list_of_weights: list):\n",
    "    agg_weights = []\n",
    "    for layer_index in range(len(list_of_weights[0])):\n",
    "        weights_per_layer = [weights[layer_index] for weights in list_of_weights]\n",
    "        weights_per_layer = tl.stack(weights_per_layer)\n",
    "        agg_layer = tl.mean(weights_per_layer, axis=0)\n",
    "        agg_weights.append(agg_layer)\n",
    "    return agg_weights\n",
    "\n",
    "\n",
    "# Aggregate weights\n",
    "aggregators.map(aggregate_with_fedavg)\n",
    "\n",
    "@set_aggregated_weights\n",
    "def set_agreggated_weights_to_server(server_flex_model: FlexModel, aggregated_weights):\n",
    "    with torch.no_grad():\n",
    "        weight_dict = server_flex_model[\"model\"].state_dict()\n",
    "        for layer_key, new in zip(weight_dict, aggregated_weights):\n",
    "            weight_dict[layer_key].copy_(new)\n",
    "\n",
    "\n",
    "aggregators.map(set_agreggated_weights_to_server, servers)\n",
    "\n",
    "\n",
    "def evaluate_global_model(server_flex_model: FlexModel, test_data: Dataset):\n",
    "    model = server_flex_model[\"model\"]\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    total_count = 0\n",
    "    model = model.to(device)\n",
    "    criterion = server_flex_model[\"criterion\"]\n",
    "    # get test data as a torchvision object\n",
    "    test_dataset = test_data.to_torchvision_dataset(transform=mnist_transforms)\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset, batch_size=256, shuffle=True, pin_memory=False\n",
    "    )\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_dataloader:\n",
    "            total_count += target.size(0)\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            losses.append(criterion(output, target).item())\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            test_acc += pred.eq(target.data.view_as(pred)).long().cpu().sum().item()\n",
    "\n",
    "    test_loss = sum(losses) / len(losses)\n",
    "    test_acc /= total_count\n",
    "    return test_loss, test_acc\n",
    "\n",
    "\n",
    "metrics = servers.map(evaluate_global_model)\n",
    "print(metrics[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is quite some code, but we can summarize it in the following steps:\n",
    "- From a dataset, we are able to federate it and create a pool from it. In our case the loaded dataset is already federated.\n",
    "- We create a `FlexModel` instance for the server which contains all necesary variables for the training and evaluation.\n",
    "- The server's model is copied to clients.\n",
    "- Clients train their model with their data.\n",
    "- Weights are collected by the server.\n",
    "- The server/aggregator aggregates all the weights.\n",
    "- The weights are set to the server and evaluated.\n",
    "- Restart from second point until convergence is reached.\n",
    "\n",
    "Now let's explore how to distribute this code.\n",
    "\n",
    "## Distributing the local experiment\n",
    "FLEXible supports (at the moment of writing) a real distributed Client-Server architecture. We will start with the client first. A FLEXible client is an instance of a subclass of the abstract class `flex.distributed.Client`. This class already contains all necesary code for communication making the user only to implement methods showing how to train, collect and set weights and run evaluation. Still, if we decided to make a local experiment first, FLEXible provides you with a convenient `flex.distributed.ClientBuilder` which will allow us to reuse a lot of code without needing to change much. Let's see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Client.py\n",
    "# We assume that we have imported all the previous functions definitions from the local experiment.\n",
    "\n",
    "from flex.distributed import ClientBuilder\n",
    "\n",
    "# Load a dataset, in a local experiment a FedDataset is nothing but a collection of datasets. Let's pick the first one from the collection.\n",
    "dataset = flex_dataset[list(flex_dataset.keys())[0]]\n",
    "\n",
    "# We create a client by passing all necessary information to the ClientBuilder.\n",
    "# We see that we can reuse the functions that we defined in the local experiment.\n",
    "client = ClientBuilder() \\\n",
    "    .build_model(build_server_model) \\\n",
    "    .dataset(dataset) \\\n",
    "    .collect_weights(get_clients_weights) \\\n",
    "    .train(train) \\\n",
    "    .eval(evaluate_global_model, dataset) \\\n",
    "    .set_weights(set_agreggated_weights_to_server) \\\n",
    "    .build()\n",
    "    \n",
    "# Finally, we can run the client.\n",
    "client.run(address=\"you_address_here:port\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have been able to create a client in just a few lines of code. For sake of simplicity, we are using the same dataset for training (set by `.dataset`) and for evaluation (set by `.eval`). Now let's talk about the minimum changes that we should have into account.\n",
    "## Changes necessary for client code\n",
    "### Collect and set weights must be wrapped\n",
    "When we are writing the functions for our local experiments, we are always working with pools. The decorators `@collect_clients_weights` and `@set_aggregated_weights` allow us to make use of the user-defined functions for the whole pool. Still, in a real distributed scenario, we have no pool object. The client builder exploits these decorators so it is able to recover the original function. This is an opinionated decision but since most of the users will define these functions with the provided decorators, it is also the most convenient.\n",
    "### Training and evaluation functions returns dictionaries\n",
    "In order to recover in the server side any information about training or evaluation such as accuracy or loss, these functions must return a dictionary with strings as keys and float as values. This is enforced by the communication protocol used. In case we do not return anything, then an empty dictionary will be always returned.\n",
    "\n",
    "### Set weights recieves an array of numpy arrays\n",
    "In a local experiment, since we never leave the computer, we tend to keep always weights on device such as GPU. Unfortunaly, network communication imposes the necessity of moving the weights to be sent to the CPU. When we are collecting weights, this conversion is transparent to the user, but when recieving weights through the network (that is, when `set_weights` is being called) we are unable to convert to any other kind of array since FLEXible is framework agnostic. This way, is responsability of the user to convert this weights to the desired tensor type, such as `torch.Tensor`.\n",
    "\n",
    "\n",
    "## The server code\n",
    "Moving on, we show how the server code works. It is different from the local code but still very simple to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Server.py\n",
    "\n",
    "from flex.distributed import Server\n",
    "\n",
    "# We need to create the server and storing it in a variable.\n",
    "server = Server()\n",
    "\n",
    "# Start the server.\n",
    "server.run(address=\"your_address_here\", port=8080)\n",
    "\n",
    "# Now we are able to use the server to communicate with clients.\n",
    "# We are able to see the total number of clients connected to the server.\n",
    "clients_connected = len(server)\n",
    "\n",
    "# To retrieve their ids.\n",
    "clients_ids = server.get_ids()\n",
    "\n",
    "# Now we may select a given ammoount of clients to run the FL process.\n",
    "number_of_clients = 10\n",
    "selected_ids = clients_ids[:number_of_clients]\n",
    "\n",
    "# We can now tell the clients to start training.\n",
    "# By passing the selected ids, the server will only communicate with the selected clients.\n",
    "metrics = server.train(node_ids=selected_ids)\n",
    "\n",
    "# Let's see the metrics.\n",
    "for node_id in metrics:\n",
    "    print(f\"Client with id {node_id} has sent the following metrics: {metrics[node_id]}\")\n",
    "    \n",
    "# Running evaluation is equivalent.\n",
    "metrics = server.eval(node_ids=selected_ids)\n",
    "\n",
    "# Now, we can also get the weights from the clients.\n",
    "weights = server.collect_weights(node_ids=selected_ids)\n",
    "\n",
    "# This weights can be aggregated\n",
    "aggregate_weights = aggregate_with_fedavg.__wrapped__(weights)\n",
    "\n",
    "# And then send the aggregated weights to the clients.\n",
    "server.send_weights(aggregate_weights, node_ids=selected_ids)\n",
    "\n",
    "# Finally, we can stop the server.\n",
    "# This is very important to avoid memory leaks.\n",
    "server.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the Server API allow us to fetch information from clients, providing us with a simple interface to use. The only important thing to notice here is that we must call `server.stop()` at the end of the code. This is necessary since `server.run()` starts multiple threads that will hang until we call this method.\n",
    "\n",
    "\n",
    "Also FLEX provides an alternative Server API based on `asyncio` and thus the `async/await` syntax. This API is not only more efficient but also allows to run operations with timeout. The API is mostly the same and can be found on `flex.distributed.aio.Server`. Here we can see the previous server code with this API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Server_aio.py\n",
    "import asyncio\n",
    "\n",
    "from flex.distributed.aio import Server\n",
    "\n",
    "async def run_server():\n",
    "    # We need to create the server and storing it in a variable.\n",
    "    server = Server()\n",
    "\n",
    "    # Start the server.\n",
    "    await server.run(address=\"your_address_here\", port=8080)\n",
    "\n",
    "    # Now we are able to use the server to communicate with clients.\n",
    "    # We are able to see the total number of clients connected to the server.\n",
    "    clients_connected = len(server)\n",
    "\n",
    "    # Also we can wait for getting some ammount of clients connected\n",
    "    await server.wait_for_clients(10)\n",
    "\n",
    "    # To retrieve their ids.\n",
    "    clients_ids = server.get_ids()\n",
    "\n",
    "    # Now we may select a given ammoount of clients to run the FL process.\n",
    "    number_of_clients = 10\n",
    "    selected_ids = clients_ids[:number_of_clients]\n",
    "\n",
    "    # We can now tell the clients to start training.\n",
    "    # By passing the selected ids, the server will only communicate with the selected clients.\n",
    "    metrics = await server.train(node_ids=selected_ids)\n",
    "\n",
    "    # Let's see the metrics.\n",
    "    for node_id in metrics:\n",
    "        print(f\"Client with id {node_id} has sent the following metrics: {metrics[node_id]}\")\n",
    "        \n",
    "    # Running evaluation is equivalent.\n",
    "    metrics = await server.eval(node_ids=selected_ids)\n",
    "\n",
    "    # Now, we can also get the weights from the clients.\n",
    "    weights = await server.collect_weights(node_ids=selected_ids)\n",
    "\n",
    "    # This weights can be aggregated\n",
    "    aggregate_weights = aggregate_with_fedavg.__wrapped__(weights)\n",
    "\n",
    "    # And then send the aggregated weights to the clients.\n",
    "    await server.send_weights(aggregate_weights, node_ids=selected_ids)\n",
    "\n",
    "    # Finally, we can stop the server.\n",
    "    # This is very important to avoid memory leaks.\n",
    "    await server.stop()\n",
    "\n",
    "# Run with asyncio\n",
    "asyncio.run(run_server())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
